#!/usr/bin/env bash
set -euo pipefail

# Configuration
DEFAULT_FLAGS=()
readonly IMAGE_NAME="claudebox"
readonly DOCKER_USER="claude"
readonly USER_ID=$(id -u)
readonly GROUP_ID=$(id -g)
readonly PROJECT_DIR="$(pwd)"

# Cross-platform script path resolution
get_script_path() {
    local source="${BASH_SOURCE[0]:-$0}"
    while [[ -L "$source" ]]; do
        local dir="$(cd -P "$(dirname "$source")" && pwd)"
        source="$(readlink "$source")"
        [[ $source != /* ]] && source="$dir/$source"
    done
    echo "$(cd -P "$(dirname "$source")" && pwd)/$(basename "$source")"
}
readonly SCRIPT_PATH="$(get_script_path)"

readonly CLAUDE_DATA_DIR="$HOME/.claude"
readonly LINK_TARGET="$HOME/.local/bin/claudebox"
readonly NODE_VERSION="--lts"

# Default flags - comment/uncomment as needed
#DEFAULT_FLAGS+=("--dangerously-skip-permissions")
#DEFAULT_FLAGS+=("--dangerously-enable-sudo")
#DEFAULT_FLAGS+=("--dangerously-disable-firewall")

# Color codes
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly PURPLE='\033[0;35m'
readonly CYAN='\033[0;36m'
readonly WHITE='\033[1;37m'
readonly NC='\033[0m'

# Utility functions
cecho() { echo -e "${2:-$NC}$1${NC}"; }
error() { cecho "$1" "$RED" >&2; exit "${2:-1}"; }
warn() { cecho "$1" "$YELLOW"; }
info() { cecho "$1" "$BLUE"; }
success() { cecho "$1" "$GREEN"; }

# Parse early flags
VERBOSE=false
for arg in "$@"; do
    case "$arg" in
        -v|--verbose) VERBOSE=true ;;
    esac
done

# Logo
logo() {
    local cb='
 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù
‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù
‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
 ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó ------ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïù ------ ‚îÇ The Ultimate ‚îÇ
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ïî‚ïù  ------ ‚îÇ Claude Code  ‚îÇ
‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó  ------ ‚îÇ  Docker Dev  ‚îÇ
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ïó ------ ‚îÇ Environment  ‚îÇ
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ------ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
'
    while IFS= read -r l; do
        o="" c=""
        for ((i=0;i<${#l};i++)); do
            ch="${l:$i:1}"
            [[ "$ch" == " " ]] && { o+="$ch"; continue; }
            cc=$(printf '%d' "'$ch" 2>/dev/null||echo 0)
            if [[ $cc -ge 32 && $cc -le 126 ]]; then n='\033[33m'
            elif [[ $cc -ge 9552 && $cc -le 9580 ]]; then n='\033[34m'
            elif [[ $cc -eq 9608 ]]; then n='\033[31m'
            else n='\033[37m'; fi
            [[ "$n" != "$c" ]] && { o+="$n"; c="$n"; }
            o+="$ch"
        done
        echo -e "${o}\033[0m"
    done <<< "$cb"
}

# Symlink management
update_symlink() {
    if [[ -L "$LINK_TARGET" ]]; then
        local current_target
        current_target=$(readlink "$LINK_TARGET" 2>/dev/null || echo "")
        if [[ "$current_target" != "$SCRIPT_PATH" ]]; then
            rm -f "$LINK_TARGET"
            ln -s "$SCRIPT_PATH" "$LINK_TARGET"
            info "Updated claudebox symlink to current location"
        fi
    elif [[ ! -e "$LINK_TARGET" ]]; then
        mkdir -p "$(dirname "$LINK_TARGET")"
        if ln -s "$SCRIPT_PATH" "$LINK_TARGET" 2>/dev/null; then
            success "Created claudebox symlink in $(dirname "$LINK_TARGET")"
        else
            warn "Note: Could not create symlink (needs sudo)"
            warn "Run with sudo or add $(dirname "$LINK_TARGET") to your PATH"
        fi
    fi
}

# Docker checks
check_docker() {
    command -v docker &>/dev/null || return 1
    docker info &>/dev/null || return 2
    docker ps &>/dev/null || return 3
    return 0
}

install_docker() {
    warn "Docker is not installed."
    cecho "Would you like to install Docker now? (y/n)" "$CYAN"
    read -r response
    [[ "$response" =~ ^[Yy]$ ]] || error "Docker is required. Visit: https://docs.docker.com/engine/install/"

    info "Installing Docker..."

    [[ -f /etc/os-release ]] && . /etc/os-release || error "Cannot detect OS"

    case "${ID:-}" in
        ubuntu|debian)
            warn "Installing Docker requires sudo privileges..."
            sudo apt-get update
            sudo apt-get install -y ca-certificates curl gnupg lsb-release
            sudo mkdir -p /etc/apt/keyrings
            curl -fsSL "https://download.docker.com/linux/$ID/gpg" | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
            echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/$ID $(lsb_release -cs) stable" | \
                sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
            sudo apt-get update
            sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
            ;;
        fedora|rhel|centos)
            warn "Installing Docker requires sudo privileges..."
            sudo dnf -y install dnf-plugins-core
            sudo dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo
            sudo dnf install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
            sudo systemctl start docker
            sudo systemctl enable docker
            ;;
        arch|manjaro)
            warn "Installing Docker requires sudo privileges..."
            sudo pacman -S --noconfirm docker
            sudo systemctl start docker
            sudo systemctl enable docker
            ;;
        *)
            error "Unsupported OS: ${ID:-unknown}. Visit: https://docs.docker.com/engine/install/"
            ;;
    esac

    success "Docker installed successfully!"
    configure_docker_nonroot
}

configure_docker_nonroot() {
    warn "Configuring Docker for non-root usage..."
    warn "This requires sudo to add you to the docker group..."

    getent group docker >/dev/null || sudo groupadd docker
    sudo usermod -aG docker "$USER"

    success "Docker configured for non-root usage!"
    warn "You need to log out and back in for group changes to take effect."
    warn "Or run: ${CYAN}newgrp docker"
    warn "Then run 'claudebox' again."
    info "Trying to activate docker group in current shell..."
    exec newgrp docker
}

# Spinner
show_spinner() {
    local pid=$1 msg=$2 spin='‚†ã‚†ô‚†π‚†∏‚†º‚†¥‚†¶‚†ß‚†á‚†è' i=0
    echo -n "$msg "
    while kill -0 "$pid" 2>/dev/null; do
        printf "\b%s" "${spin:i++%${#spin}:1}"
        sleep 0.1
    done
    echo -e "\b${GREEN}‚úì${NC}"
}

# Profile definitions
declare -A PROFILES PROFILE_DESCRIPTIONS
PROFILES[c]="build-essential gcc g++ gdb valgrind cmake ninja-build clang clang-format clang-tidy cppcheck doxygen libboost-all-dev autoconf automake libtool pkg-config libcmocka-dev libcmocka0 lcov libncurses5-dev libncursesw5-dev"
PROFILE_DESCRIPTIONS[c]="C/C++ Development (compilers, debuggers, analyzers, build tools, cmocka, coverage, ncurses)"
PROFILES[openwrt]="build-essential gcc g++ make git wget unzip sudo file python3 python3-distutils rsync libncurses5-dev zlib1g-dev gawk gettext libssl-dev xsltproc libelf-dev libtool automake autoconf ccache subversion swig time qemu-system-arm qemu-system-aarch64 qemu-system-mips qemu-system-x86 qemu-utils"
PROFILE_DESCRIPTIONS[openwrt]="OpenWRT Development (cross-compilation, QEMU, build essentials)"
PROFILES[rust]="curl build-essential pkg-config libssl-dev"
PROFILE_DESCRIPTIONS[rust]="Rust Development (cargo and rustc will be installed separately)"
PROFILES[python]="python3 python3-pip python3-venv python3-dev build-essential libffi-dev libssl-dev"
PROFILE_DESCRIPTIONS[python]="Python Development (Python 3, pip, venv, build tools)"
PROFILES[go]="wget git build-essential"
PROFILE_DESCRIPTIONS[go]="Go Development (Go will be installed separately)"
PROFILES[javascript]="build-essential python3"
PROFILE_DESCRIPTIONS[javascript]="JavaScript/TypeScript Development (Node.js, npm, yarn)"
PROFILES[java]="openjdk-17-jdk maven gradle ant"
PROFILE_DESCRIPTIONS[java]="Java Development (OpenJDK 17, Maven, Gradle, Ant)"
PROFILES[ruby]="ruby-full ruby-dev build-essential zlib1g-dev libssl-dev libreadline-dev libyaml-dev libsqlite3-dev sqlite3 libxml2-dev libxslt1-dev libcurl4-openssl-dev software-properties-common libffi-dev"
PROFILE_DESCRIPTIONS[ruby]="Ruby Development (Ruby, gems, build tools)"
PROFILES[php]="php php-cli php-fpm php-mysql php-pgsql php-sqlite3 php-curl php-gd php-mbstring php-xml php-zip composer"
PROFILE_DESCRIPTIONS[php]="PHP Development (PHP, Composer, common extensions)"
PROFILES[database]="postgresql-client mysql-client sqlite3 redis-tools mongodb-clients"
PROFILE_DESCRIPTIONS[database]="Database Tools (PostgreSQL, MySQL, SQLite, Redis, MongoDB clients)"
PROFILES[devops]="docker.io docker-compose kubectl helm terraform ansible awscli"
PROFILE_DESCRIPTIONS[devops]="DevOps Tools (Docker, K8s, Terraform, Ansible, AWS CLI)"
PROFILES[web]="nginx apache2-utils curl wget httpie jq"
PROFILE_DESCRIPTIONS[web]="Web Development Tools (nginx, curl, httpie, jq)"
PROFILES[embedded]="gcc-arm-none-eabi gdb-multiarch openocd picocom minicom screen platformio"
PROFILE_DESCRIPTIONS[embedded]="Embedded Development (ARM toolchain, debuggers, serial tools)"
PROFILES[datascience]="python3-numpy python3-pandas python3-scipy python3-matplotlib python3-seaborn jupyter-notebook r-base"
PROFILE_DESCRIPTIONS[datascience]="Data Science (NumPy, Pandas, Jupyter, R)"
PROFILES[security]="nmap tcpdump wireshark-common netcat-openbsd john hashcat hydra"
PROFILE_DESCRIPTIONS[security]="Security Tools (network analysis, penetration testing)"
PROFILES[ml]="python3-pip python3-dev python3-venv build-essential cmake"
PROFILE_DESCRIPTIONS[ml]="Machine Learning (base tools, Python libs installed separately)"

# Docker operations
docker_exec_root() {
    docker exec -u root "$@"
}

docker_exec_user() {
    docker exec -u "$DOCKER_USER" "$@"
}

# Helper functions for project profiles
get_project_folder_name() {
    echo "$1" | sed 's|^/||; s|/|-|g'
}

get_profile_file_path() {
    local profile_dir="$HOME/.claudebox/profiles"
    mkdir -p "$profile_dir"
    echo "$profile_dir/$(echo "$PROJECT_DIR" | sed 's|/|-|g' | sed 's|^-||').ini"
}

read_profile_section() {
    local profile_file="$1"
    local section="$2"
    local result=()

    if [[ -f "$profile_file" ]] && grep -q "^\[$section\]" "$profile_file"; then
        while IFS= read -r line; do
            [[ -z "$line" || "$line" =~ ^\[.*\]$ ]] && break
            result+=("$line")
        done < <(sed -n "/^\[$section\]/,/^\[/p" "$profile_file" | tail -n +2 | grep -v '^\[')
    fi

    printf '%s\n' "${result[@]}"
}

update_profile_section() {
    local profile_file="$1"
    local section="$2"
    shift 2
    local new_items=("$@")

    # Read existing items
    local existing_items=()
    readarray -t existing_items < <(read_profile_section "$profile_file" "$section")

    # Merge with new items (avoid duplicates)
    local all_items=()
    for item in "${existing_items[@]}"; do
        [[ -n "$item" ]] && all_items+=("$item")
    done

    for item in "${new_items[@]}"; do
        local found=false
        for existing in "${all_items[@]}"; do
            [[ "$existing" == "$item" ]] && found=true && break
        done
        [[ "$found" == "false" ]] && all_items+=("$item")
    done

    # Write updated profile file
    {
        # Preserve other sections
        if [[ -f "$profile_file" ]]; then
            awk -v sect="$section" '
                BEGIN { in_section=0; skip_section=0 }
                /^\[/ {
                    if ($0 == "[" sect "]") { skip_section=1; in_section=1 }
                    else { skip_section=0; in_section=0 }
                }
                !skip_section { print }
                /^\[/ && !skip_section && in_section { in_section=0 }
            ' "$profile_file"
        fi

        # Add our section
        echo "[$section]"
        for item in "${all_items[@]}"; do
            echo "$item"
        done
        echo ""
    } > "${profile_file}.tmp" && mv "${profile_file}.tmp" "$profile_file"
}

# Project-specific folder setup
setup_project_folder() {
    local project_folder_name
    project_folder_name=$(get_project_folder_name "$PROJECT_DIR")
    PROJECT_CLAUDEBOX_DIR="$HOME/.claudebox/$project_folder_name"

    mkdir -p "$PROJECT_CLAUDEBOX_DIR/claude-config"
    mkdir -p "$PROJECT_CLAUDEBOX_DIR/memory"
    mkdir -p "$PROJECT_CLAUDEBOX_DIR/context"
    mkdir -p "$PROJECT_CLAUDEBOX_DIR/firewall"
}

# Global MCP configuration setup
setup_global_mcp_config() {
    local mcp_file="$HOME/.claudebox/.mcp.json"
    local mcp_content='{
  "mcpServers": {
    "memory": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-memory"]
    },
    "sequential-thinking": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-sequential-thinking"]
    },
    "context7": {
      "command": "npx",
      "args": ["-y", "@upstash/context7-mcp"]
    }
  }
}'

    mkdir -p "$HOME/.claudebox"

    if [[ ! -f "$mcp_file" ]]; then
        echo "$mcp_content" > "$mcp_file"
        echo -e "${GREEN}Created MCP config in ~/.claudebox/.mcp.json${NC}"
    fi
}

# Setup Claude agent command
setup_claude_agent_command() {
    mkdir -p .claude/commands
    cat << 'EOF' > .claude/commands/agent.md
# Agentic Loop Framework
<System>
You are building an **Agentic Loop** that can tackle any complex task with minimal role bloat.

**Core principles**

1. **Single-brain overview** ‚Äì¬†One Orchestrator owns the big picture.
2. **Few, powerful agents** ‚Äì¬†Reuse the same Specialist prompt for parallelism instead of inventing many micro-roles.
3. **Tight feedback** ‚Äì¬†A dedicated Evaluator grades outputs (0-100) and suggests concrete fixes until quality ‚â•¬†TARGET_SCORE.
4. **Shared context** ‚Äì¬†Every agent receives the same `context.md` so no information is siloed.
5. **Repo-aware** ‚Äì¬†The Orchestrator decides whether to align to the current repo or create a generic loop.
6. **Explicit imperatives** ‚Äì¬†Use the labels **‚ÄúYou Must‚Äù** or **‚ÄúImportant‚Äù** for non-negotiable steps; permit extra compute with **‚ÄúThink hard‚Äù** / **‚Äúultrathink‚Äù**.

</System>

<Context>
**Task**:¬†<<USER_DESCRIBED_TASK>>
**Repo path (if any)**:¬†<<ABSOLUTE_PATH_OR_NONE>>
**Desired parallelism**:¬†<<N_PARALLEL_SPECIALISTS>> ¬†(1-3 is typical)

The Orchestrator must decide:

- Whether to specialise the workflow to this repo or keep it generic.
- How many identical Specialist instances to launch (0¬†= sequential).
</Context>

<Instructions>

### 0  Bootstrap
- **You Must** create `./docs/<TASK>/context.md` containing this entire block so all agents share it.

### 1  Orchestrator
```

# Orchestrator ‚Äî codename ‚ÄúAtlas‚Äù

You coordinate everything.

You Must:

1. Parse `context.md`.
2. Decide repo-specific vs generic flow.
3. Spawn N parallel **Specialist** agents with shared context.

   * If N > 1, allocate sub-tasks or file patches to avoid merge conflicts.
4. After Specialists finish, send their outputs to the **Evaluator**.
5. If Evaluator‚Äôs score < TARGET\_SCORE (default = 90), iterate:
   a. Forward feedback to Specialists.
   b. **Think hard** and relaunch refined tasks.
6. On success, run the *Consolidate* step (below) and write the final artefacts to
   `./outputs/<TASK>_<TIMESTAMP>/final/`.
   Important: **Never** lose or overwrite an agent‚Äôs original markdown; always copy to `/phaseX/`.

```

### 2  Specialist
```

# Specialist ‚Äî codename ‚ÄúMercury‚Äù

Role: A multi-disciplinary expert who can research, code, write, and test.

Input: full `context.md` plus Orchestrator commands.
Output: Markdown file in `/phaseX/` that fully addresses your assigned slice.

You Must:

1. Acknowledge uncertainties; request missing info instead of hallucinating.
2. Use TDD if coding: write failing unit tests first, then code till green.
3. Tag heavyweight reasoning with **ultrathink** (visible to Evaluator).
4. Deliver clean, self-contained markdown.

```

### 3  Evaluator
```

# Evaluator ‚Äî codename ‚ÄúApollo‚Äù

Role: Critically grade each Specialist bundle.

Input: Specialist outputs.
Output: A file `evaluation_phaseX.md` containing:

* Numeric score 0-100
* Up to 3 strengths
* Up to 3 issues
* Concrete fix suggestions
  Verdict: `APPROVE` ‚ñºor `ITERATE`.
  You Must be specific and ruthless; no rubber-stamping.

```

### 4  Consolidate (Orchestrator-run)
```

You Must merge approved Specialist outputs, remove duplication, and ensure:

* Consistent style
* All referenced files exist
* README or equivalent final deliverable is complete

```
</Instructions>

<Constraints>
- Keep total roles fixed at **three** (Atlas, Mercury, Apollo).
- Avoid unnecessary follow-up questions; ask only if a missing piece blocks progress.
- Use markdown only; no emojis or decorative unicode.
- Absolute paths, filenames, and directory layout must match reality.
</Constraints>

<Output Format>
```

‚úÖ Created/updated: ./docs/<TASK>/context.md
‚úÖ Created/updated: ./.claude/commands/<TASK>.md   # Orchestrator
‚úÖ Created/updated: ./docs/<TASK>/specialist.md    # Mercury
‚úÖ Created/updated: ./docs/<TASK>/evaluator.md     # Apollo

üìÅ Runtime outputs: ./outputs/<TASK>\_<TIMESTAMP>/

```
</Output Format>

<User Input>
```

What is the topic or role of the agent loop you want to create?
Provide any necessary details and I‚Äôll generate the minimal, high-fidelity workflow.

```
</User Input>
```
EOF
}

# Create files to copy into the image
create_build_files() {
    local build_context="$1"

    cat > "$build_context/init-firewall.sh" << 'EOF'
#!/bin/bash
set -euo pipefail
if [ "${DISABLE_FIREWALL:-false}" = "true" ]; then
    echo "Firewall disabled, skipping setup"
    rm -f "$0"
    exit 0
fi
iptables -F OUTPUT 2>/dev/null || true
iptables -F INPUT 2>/dev/null || true
iptables -A OUTPUT -p udp --dport 53 -j ACCEPT
iptables -A OUTPUT -p tcp --dport 53 -j ACCEPT
iptables -A INPUT -p udp --sport 53 -j ACCEPT
iptables -A OUTPUT -o lo -j ACCEPT
iptables -A INPUT -i lo -j ACCEPT
iptables -A OUTPUT -s 127.0.0.0/8 -d 127.0.0.0/8 -j ACCEPT
iptables -A INPUT -s 127.0.0.0/8 -d 127.0.0.0/8 -j ACCEPT
iptables -A OUTPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
# Default allowed domains
DEFAULT_DOMAINS="api.anthropic.com console.anthropic.com statsig.anthropic.com sentry.io"

# Read additional domains from allowlist file if it exists
ALLOWED_DOMAINS="$DEFAULT_DOMAINS"
# Try project-specific allowlist first, then fall back to global
if [ -f /home/DOCKERUSER/.claudebox-project/firewall/allowlist ]; then
    echo "Loading project-specific allowlist"
    while IFS= read -r domain; do
        # Skip comments and empty lines
        [[ "$domain" =~ ^#.* ]] && continue
        [[ -z "$domain" ]] && continue
        # Remove wildcards for now (*.example.com becomes example.com)
        domain="${domain#\*.}"
        ALLOWED_DOMAINS="$ALLOWED_DOMAINS $domain"
    done < /home/DOCKERUSER/.claudebox-project/firewall/allowlist
elif [ -f /home/DOCKERUSER/.claudebox/allowlist ]; then
    while IFS= read -r domain; do
        # Skip comments and empty lines
        [[ "$domain" =~ ^#.* ]] && continue
        [[ -z "$domain" ]] && continue
        # Remove wildcards for now (*.example.com becomes example.com)
        domain="${domain#\*.}"
        ALLOWED_DOMAINS="$ALLOWED_DOMAINS $domain"
    done < /home/DOCKERUSER/.claudebox/allowlist
fi

if command -v ipset >/dev/null 2>&1; then
    ipset destroy allowed-domains 2>/dev/null || true
    ipset create allowed-domains hash:net
    ipset destroy allowed-ips 2>/dev/null || true
    ipset create allowed-ips hash:net

    for domain in $ALLOWED_DOMAINS; do
        # Check if it's an IP range
        if [[ "$domain" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+/[0-9]+$ ]]; then
            ipset add allowed-ips $domain 2>/dev/null || true
        else
            # It's a domain, resolve it
            ips=$(getent hosts $domain 2>/dev/null | awk '{print $1}')
            for ip in $ips; do
                ipset add allowed-domains $ip 2>/dev/null || true
            done
        fi
    done
    iptables -A OUTPUT -m set --match-set allowed-domains dst -j ACCEPT
    iptables -A OUTPUT -m set --match-set allowed-ips dst -j ACCEPT
else
    # Fallback without ipset
    for domain in $ALLOWED_DOMAINS; do
        if [[ "$domain" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+/[0-9]+$ ]]; then
            iptables -A OUTPUT -d $domain -j ACCEPT
        else
            # Resolve domain to IPs
            ips=$(getent hosts $domain 2>/dev/null | awk '{print $1}')
            for ip in $ips; do
                iptables -A OUTPUT -d $ip -j ACCEPT
            done
        fi
    done
fi
iptables -P OUTPUT DROP
iptables -P INPUT DROP
echo "Firewall initialized with Anthropic-only access"
rm -f "$0"
EOF

    cat > "$build_context/claude-wrapper" << 'EOF'
#!/bin/bash
export NVM_DIR="$HOME/.nvm"
[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"
nvm use default >/dev/null 2>&1
# Use MCP config if it exists
if [ -f "$HOME/.claudebox/.mcp.json" ]; then
    exec claude --mcp-config "$HOME/.claudebox/.mcp.json" "$@"
else
    exec claude "$@"
fi
EOF

    cat > "$build_context/docker-entrypoint.sh" << 'EOF'
#!/bin/bash
set -euo pipefail
ENABLE_SUDO=false
DISABLE_FIREWALL=false
while [[ $# -gt 0 ]]; do
    case "$1" in
        --dangerously-enable-sudo) ENABLE_SUDO=true; shift ;;
        --dangerously-disable-firewall) DISABLE_FIREWALL=true; shift ;;
        *) break ;;
    esac
done
export DISABLE_FIREWALL

# Set up project-specific symlinks
if [ -d /home/DOCKERUSER/.claudebox-project ]; then
    # Ensure memory directory is accessible
    mkdir -p /home/DOCKERUSER/.claudebox-project/memory
    chown -R DOCKERUSER:DOCKERUSER /home/DOCKERUSER/.claudebox-project
fi

if [ -f /home/DOCKERUSER/init-firewall.sh ]; then
    /home/DOCKERUSER/init-firewall.sh || true
fi
if [ "$ENABLE_SUDO" = "true" ]; then
    echo "DOCKERUSER ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/DOCKERUSER
    chmod 0440 /etc/sudoers.d/DOCKERUSER
fi

if [ -n "$CLAUDEBOX_PROJECT_DIR" ]; then
    PROFILE_FILE="/home/DOCKERUSER/.claudebox/profiles/$(echo "$CLAUDEBOX_PROJECT_DIR" | sed 's|/|-|g' | sed 's|^-||').ini"

    if command -v uv >/dev/null 2>&1 && [ -f "$PROFILE_FILE" ] && grep -q "python" "$PROFILE_FILE"; then
        if [ ! -d /home/DOCKERUSER/.claudebox-project/.venv ]; then
            su - DOCKERUSER -c "uv venv /home/DOCKERUSER/.claudebox-project/.venv"
            if [ -f /workspace/pyproject.toml ]; then
                su - DOCKERUSER -c "cd /workspace && uv sync"
            else
                su - DOCKERUSER -c "uv pip install --python /home/DOCKERUSER/.claudebox-project/.venv/bin/python ipython black pylint mypy flake8 pytest ruff"
            fi
        fi

        for shell_rc in /home/DOCKERUSER/.zshrc /home/DOCKERUSER/.bashrc; do
            if ! grep -q "source /home/DOCKERUSER/.claudebox-project/.venv/bin/activate" "$shell_rc"; then
                echo 'if [ -f /home/DOCKERUSER/.claudebox-project/.venv/bin/activate ]; then source /home/DOCKERUSER/.claudebox-project/.venv/bin/activate; fi' >> "$shell_rc"
            fi
        done
    fi
fi

cd /home/DOCKERUSER
# Check if we're in shell mode
if [[ "$1" == "--shell-mode" ]]; then
    shift  # Remove --shell-mode flag
    exec su DOCKERUSER -c "source /home/DOCKERUSER/.nvm/nvm.sh && cd /workspace && exec /bin/zsh"
else
    # Build command with properly quoted arguments
    cmd="cd /workspace && /home/DOCKERUSER/claude-wrapper"
    for arg in "$@"; do
        # Escape single quotes in the argument
        escaped_arg=$(echo "$arg" | sed "s/'/'\\\\''/g")
        cmd="$cmd '$escaped_arg'"
    done
    exec su DOCKERUSER -c "$cmd"
fi
EOF
}

run_docker_build() {
    info "Running docker build..."
    docker build \
        --build-arg USER_ID="$USER_ID" \
        --build-arg GROUP_ID="$GROUP_ID" \
        --build-arg USERNAME="$DOCKER_USER" \
        --build-arg NODE_VERSION="$NODE_VERSION" \
        -f "$1" -t "$IMAGE_NAME" "$2"
}

# Main execution
main() {
    update_symlink

    # Check Docker
    local docker_status
    docker_status=$(check_docker; echo $?)
    case $docker_status in
        1) install_docker ;;
        2)
            warn "Docker is installed but not running."
            warn "Starting Docker requires sudo privileges..."
            sudo systemctl start docker
            docker info &>/dev/null || error "Failed to start Docker"
            docker ps &>/dev/null || configure_docker_nonroot
            ;;
        3)
            warn "Docker requires sudo. Setting up non-root access..."
            configure_docker_nonroot
            ;;
    esac

    # Handle commands
    [[ "$VERBOSE" == "true" ]] && echo "Command: ${1:-none}" >&2
    case "${1:-}" in
        profile)
            shift
            if [[ $# -eq 0 ]]; then
                cecho "Available Profiles:" "$CYAN"
                echo
                for profile in $(printf '%s\n' "${!PROFILE_DESCRIPTIONS[@]}" | sort); do
                    echo -e "  ${GREEN}$profile${NC} - ${PROFILE_DESCRIPTIONS[$profile]}"
                done
                echo
                warn "Usage: claudebox profile <name> [<name2> ...]"
                warn "Example: claudebox profile c python web"
                exit 0
            fi

            local selected=() remaining=()
            while [[ $# -gt 0 ]]; do
                if [[ -n "${PROFILES[$1]:-}" ]]; then
                    selected+=("$1")
                    shift
                else
                    remaining=("$@")
                    break
                fi
            done

            [[ ${#selected[@]} -eq 0 ]] && error "No valid profiles specified\nRun 'claudebox profile' to see available profiles"

            local profile_file
            profile_file=$(get_profile_file_path)

            # Update profiles section
            update_profile_section "$profile_file" "profiles" "${selected[@]}"

            # Read all current profiles for display
            local all_profiles=()
            readarray -t all_profiles < <(read_profile_section "$profile_file" "profiles")

            cecho "Profile: $PROJECT_DIR" "$CYAN"
            cecho "Installing profiles: ${selected[*]}" "$PURPLE"
            if [[ ${#all_profiles[@]} -gt 0 ]]; then
                cecho "All active profiles: ${all_profiles[*]}" "$GREEN"
            fi
            echo

            # Continue to main execution to create container with profiles
            if [[ ${#remaining[@]} -gt 0 ]]; then
                set -- "${remaining[@]}"
            fi
            ;;

        install)
            shift
            [[ $# -eq 0 ]] && error "No packages specified. Usage: claudebox install <package1> <package2> ..."

            local profile_file
            profile_file=$(get_profile_file_path)

            # Update packages section
            update_profile_section "$profile_file" "packages" "$@"

            # Read all current packages for display
            local all_packages=()
            readarray -t all_packages < <(read_profile_section "$profile_file" "packages")

            cecho "Profile: $PROJECT_DIR" "$CYAN"
            cecho "Installing packages: $*" "$PURPLE"
            if [[ ${#all_packages[@]} -gt 0 ]]; then
                cecho "All packages: ${all_packages[*]}" "$GREEN"
            fi
            echo
            ;;

        save|commit)
            shift
            warn "The 'save' command is deprecated!"
            info "With --rm containers, there's nothing to save."
            info "Profiles and packages are tracked in profiles.json automatically."
            echo

            # Show current project's profile info
            local profile_file
            profile_file=$(get_profile_file_path)

            if [[ -f "$profile_file" ]]; then
                cecho "Current project profile info ($PROJECT_DIR):" "$CYAN"
                cat "$profile_file"
            else
                info "No profiles configured for current project."
            fi

            exit 0
            ;;

        update|config|mcp|migrate-installer)
            [[ ! -f /.dockerenv ]] && docker image inspect "$IMAGE_NAME" &>/dev/null || \
                error "ClaudeBox image not found.\nRun ${GREEN}claudebox${NC} first to build the image."

            local cmd=$1
            shift

            info "Running $cmd command..."

            # Create temporary container
            local temp_container
            temp_container=$(docker create "$IMAGE_NAME" sleep infinity)
            docker start "$temp_container" >/dev/null

            if [[ "$cmd" == "update" ]]; then
                info "Updating Claude code..."
                docker exec -u "$DOCKER_USER" "$temp_container" bash -c "
                    source \$HOME/.nvm/nvm.sh
                    nvm use default
                    claude update
                    claude --version
                "
            else
                docker exec -it -u "$DOCKER_USER" "$temp_container" \
                    /home/$DOCKER_USER/claude-wrapper "${DEFAULT_FLAGS[@]}" "$cmd" "$@"
            fi

            # Commit changes back to image
            docker commit "$temp_container" "$IMAGE_NAME" >/dev/null
            docker rm -f "$temp_container" >/dev/null

            success "$cmd completed and saved to image!"
            exit 0
            ;;

        shell)
            shift
            # Just add --shell-mode to the arguments and continue
            set -- "--shell-mode" "$@"
            ;;

        clean)
            shift
            case "${1:-}" in
                --help|-h)
                    cecho "ClaudeBox Clean Options:" "$CYAN"
                    echo
                    echo -e "  ${GREEN}clean${NC}                    Remove all containers (preserves image)"
                    echo -e "  ${GREEN}clean --project${NC}          Remove current project's data and profile"
                    echo -e "  ${GREEN}clean --all${NC}              Remove everything: containers, image, cache, symlink"
                    echo -e "  ${GREEN}clean --image${NC}            Remove containers and image (preserves build cache)"
                    echo -e "  ${GREEN}clean --cache${NC}            Remove Docker build cache only"
                    echo -e "  ${GREEN}clean --volumes${NC}          Remove associated Docker volumes"
                    echo -e "  ${GREEN}clean --symlink${NC}          Remove claudebox symlink only"
                    echo -e "  ${GREEN}clean --dangling${NC}         Remove dangling images and unused containers"
                    echo -e "  ${GREEN}clean --logs${NC}             Clear Docker container logs"
                    echo -e "  ${GREEN}clean --help${NC}             Show this help message"
                    echo
                    cecho "Examples:" "$YELLOW"
                    echo "  claudebox clean              # Remove all containers"
                    echo "  claudebox clean --project    # Remove only this project's container"
                    echo "  claudebox clean --image      # Remove containers and image"
                    echo "  claudebox clean --all        # Complete cleanup and reset"
                    exit 0
                    ;;
                --all|-a)
                    warn "Complete ClaudeBox cleanup: removing containers, image, cache, and symlink..."
                    # Remove all profile files
                    rm -rf "$HOME/.claudebox/profiles"
                    # Remove any containers from this image
                    docker ps -a --filter "ancestor=$IMAGE_NAME" -q | xargs -r docker rm -f 2>/dev/null || true
                    # Remove orphaned containers from images that no longer exist
                    # This is safer as it only removes containers whose images are gone
                    docker ps -a --filter "status=exited" --format "{{.ID}} {{.Image}}" | while read id image; do
                        if ! docker image inspect "$image" >/dev/null 2>&1; then
                            docker rm -f "$id" 2>/dev/null || true
                        fi
                    done
                    # Remove image
                    docker rmi -f "$IMAGE_NAME" 2>/dev/null || true
                    # Prune build cache
                    docker builder prune -af 2>/dev/null || true
                    # Remove volumes
                    docker volume ls -q --filter "name=claudebox" | xargs -r docker volume rm 2>/dev/null || true
                    # Remove symlink
                    [[ -L "$LINK_TARGET" ]] && rm -f "$LINK_TARGET" && info "Removed claudebox symlink"
                    success "Complete cleanup finished!"
                    ;;
                --image|-i)
                    warn "Removing ClaudeBox containers and image..."
                    # Remove all profile files
                    rm -rf "$HOME/.claudebox/profiles"
                    # Remove any containers from this image
                    docker ps -a --filter "ancestor=$IMAGE_NAME" -q | xargs -r docker rm -f 2>/dev/null || true
                    # Remove orphaned containers from images that no longer exist
                    # This is safer as it only removes containers whose images are gone
                    docker ps -a --filter "status=exited" --format "{{.ID}} {{.Image}}" | while read id image; do
                        if ! docker image inspect "$image" >/dev/null 2>&1; then
                            docker rm -f "$id" 2>/dev/null || true
                        fi
                    done
                    docker rmi -f "$IMAGE_NAME" 2>/dev/null || true
                    success "Containers and image removed! Build cache preserved."
                    ;;
                --cache|-c)
                    warn "Cleaning Docker build cache..."
                    docker builder prune -af
                    success "Build cache cleaned!"
                    ;;
                --volumes|-v)
                    warn "Removing ClaudeBox-related volumes..."
                    docker volume ls -q --filter "name=claudebox" | xargs -r docker volume rm 2>/dev/null || true
                    docker volume prune -f 2>/dev/null || true
                    success "Volumes cleaned!"
                    ;;
                --symlink|-s)
                    if [[ -L "$LINK_TARGET" ]]; then
                        rm -f "$LINK_TARGET"
                        success "Removed claudebox symlink from $(dirname "$LINK_TARGET")"
                    else
                        info "No claudebox symlink found at $LINK_TARGET"
                    fi
                    exit 0
                    ;;
                --dangling|-d)
                    warn "Removing dangling images and unused containers..."
                    docker image prune -f
                    docker container prune -f
                    success "Dangling resources cleaned!"
                    ;;
                --logs|-l)
                    warn "Clearing Docker container logs..."
                    docker ps -a --filter "ancestor=$IMAGE_NAME" -q | while read -r container; do
                        docker logs "$container" >/dev/null 2>&1 && echo -n | docker logs "$container" 2>/dev/null || true
                    done
                    success "Container logs cleared!"
                    ;;
                --project|-p)
                    warn "Removing data for current project: $PROJECT_DIR"
                    local profile_file
                    profile_file=$(get_profile_file_path)

                    # Remove profile
                    if [[ -f "$profile_file" ]]; then
                        rm -f "$profile_file"
                        success "Removed profile for $PROJECT_DIR"
                    fi

                    # Remove project-specific folder
                    local project_folder_name
                    project_folder_name=$(get_project_folder_name "$PROJECT_DIR")
                    local project_claudebox_dir="$HOME/.claudebox/$project_folder_name"

                    if [[ -d "$project_claudebox_dir" ]]; then
                        rm -rf "$project_claudebox_dir"
                        success "Removed project data folder: $project_claudebox_dir"
                    else
                        info "No project data folder found"
                    fi
                    exit 0
                    ;;
                *)
                    warn "Cleaning ClaudeBox containers..."
                    # Remove all profile files
                    rm -rf "$HOME/.claudebox/profiles"
                    # Remove any containers from this image
                    docker ps -a --filter "ancestor=$IMAGE_NAME" -q | xargs -r docker rm -f 2>/dev/null || true
                    # Remove orphaned containers from images that no longer exist
                    # This is safer as it only removes containers whose images are gone
                    docker ps -a --filter "status=exited" --format "{{.ID}} {{.Image}}" | while read id image; do
                        if ! docker image inspect "$image" >/dev/null 2>&1; then
                            docker rm -f "$id" 2>/dev/null || true
                        fi
                    done
                    success "Containers removed! Image preserved for quick recreation."
                    ;;
            esac
            echo
            docker system df
            exit 0
            ;;

        help|--help|-h)
            if docker image inspect "$IMAGE_NAME" &>/dev/null; then
                docker run --rm \
                    -u "$DOCKER_USER" \
                    --entrypoint /home/$DOCKER_USER/claude-wrapper \
                    "$IMAGE_NAME" --help | sed '1s/claude/claudebox/g'
                echo
                cecho "Added Options:" "$WHITE"
                echo -e "${CYAN}  -v, --verbose                   ${WHITE}Show detailed output"
                echo -e "${CYAN}  --dangerously-enable-sudo       ${WHITE}Enable sudo without password"
                echo -e "${CYAN}  --dangerously-disable-firewall  ${WHITE}Disable network restrictions"
                echo
                cecho "Added Commands:" "$WHITE"
                echo -e "  profile [names...]              Install language profiles"
                echo -e "  install <packages>              Install apt packages"
                echo -e "  save                            (Deprecated) Container state is auto-tracked"
                echo -e "  shell                           Open bash shell in container"
                echo -e "  info                            Show ClaudeBox container status"
                echo -e "  clean                           Clean up ClaudeBox resources (use 'clean --help' for options)"
                echo -e "  rebuild                         Rebuild the Docker image from scratch${NC}"
            else
                cecho "ClaudeBox - Claude Code Docker Environment" "$CYAN"
                echo
                warn "First run setup required!"
                echo "Run script without arguments first to build the Docker image."
            fi
            exit 0
            ;;

        info)
            shift
            cecho "ClaudeBox Profile Status" "$CYAN"
            echo

            local profile_dir="$HOME/.claudebox/profiles"
            if [[ ! -d "$profile_dir" ]] || [[ -z "$(ls -A "$profile_dir" 2>/dev/null)" ]]; then
                warn "No profiles configured yet."
                exit 0
            fi

            # Show all profiles
            local count
            count=$(ls -1 "$profile_dir"/*.ini 2>/dev/null | wc -l)
            info "Tracking $count project profile(s)"
            echo

            # Show each project's profiles
            for pfile in "$profile_dir"/*.ini; do
                [[ -f "$pfile" ]] || continue
                local proj_path
                proj_path=$(basename "$pfile" .ini | sed 's|-|/|g')
                cecho "/$proj_path:" "$YELLOW"

                # Show profiles
                local profiles=()
                readarray -t profiles < <(read_profile_section "$pfile" "profiles")
                if [[ ${#profiles[@]} -gt 0 ]]; then
                    echo "  Profiles: ${profiles[*]}"
                fi

                # Show packages
                local packages=()
                readarray -t packages < <(read_profile_section "$pfile" "packages")
                if [[ ${#packages[@]} -gt 0 ]]; then
                    echo "  Packages: ${packages[*]}"
                fi
                echo
            done

            # Show current project's profiles
            local current_profile_file
            current_profile_file=$(get_profile_file_path)
            if [[ -f "$current_profile_file" ]]; then
                cecho "Current project ($PROJECT_DIR):" "$GREEN"
                local current_profiles=()
                readarray -t current_profiles < <(read_profile_section "$current_profile_file" "profiles")
                if [[ ${#current_profiles[@]} -gt 0 ]]; then
                    echo "  Profiles: ${current_profiles[*]}"
                fi

                local current_packages=()
                readarray -t current_packages < <(read_profile_section "$current_profile_file" "packages")
                if [[ ${#current_packages[@]} -gt 0 ]]; then
                    echo "  Packages: ${current_packages[*]}"
                fi
            else
                info "Current project has no profiles configured."
            fi

            # Show running containers
            echo
            local running_containers
            running_containers=$(docker ps --filter "ancestor=$IMAGE_NAME" --format "table {{.ID}}\t{{.Status}}\t{{.Command}}" | tail -n +2)
            if [[ -n "$running_containers" ]]; then
                cecho "Running ClaudeBox containers:" "$YELLOW"
                echo "$running_containers"
            else
                info "No ClaudeBox containers currently running."
            fi

            exit 0
            ;;

        rebuild)
            shift  # Remove 'rebuild' from arguments
            warn "Rebuilding ClaudeBox Docker image..."

            # Check if image exists and remove it
            if docker image inspect "$IMAGE_NAME" &>/dev/null; then
                # Stop any running containers from this image first
                docker ps -a --filter "ancestor=$IMAGE_NAME" -q | xargs -r docker rm -f 2>/dev/null || true
                docker rmi -f "$IMAGE_NAME" 2>/dev/null || true
            fi

            # Continue to main execution which will detect missing image and rebuild
            ;;
    esac

    # Setup workspace and global config
    setup_project_folder
    setup_global_mcp_config
    setup_claude_agent_command

    # Profile tracking
    mkdir -p "$HOME/.claudebox/profiles"

    # Check if we need to rebuild based on profiles
    local need_rebuild=false
    local current_profiles=()
    local profile_hash=""

    # Collect all profiles from all projects
    if [[ -d "$HOME/.claudebox/profiles" ]]; then
        for profile_file in "$HOME/.claudebox/profiles"/*.ini; do
            [[ -f "$profile_file" ]] || continue
            local profiles_from_file=()
            readarray -t profiles_from_file < <(read_profile_section "$profile_file" "profiles")
            for profile in "${profiles_from_file[@]}"; do
                profile=$(echo "$profile" | tr -d '[:space:]')
                [[ -z "$profile" ]] && continue
                # Add to array if not already present
                local found=false
                for p in "${current_profiles[@]}"; do
                    [[ "$p" == "$profile" ]] && found=true && break
                done
                [[ "$found" == "false" ]] && current_profiles+=("$profile")
            done
        done

        # Create a hash of current profiles
        if [[ ${#current_profiles[@]} -gt 0 ]]; then
            profile_hash=$(printf '%s\n' "${current_profiles[@]}" | sort | sha256sum | cut -d' ' -f1)
        fi
    fi

    # Check if image exists and if profiles match
    if docker image inspect "$IMAGE_NAME" >/dev/null 2>&1; then
        # Get the profile hash from the image labels
        local image_profile_hash
        image_profile_hash=$(docker inspect "$IMAGE_NAME" --format '{{index .Config.Labels "claudebox.profiles"}}' 2>/dev/null || echo "")

        if [[ "$profile_hash" != "$image_profile_hash" ]]; then
            warn "Profiles have changed. Rebuilding image..."
            warn "Current profiles: ${current_profiles[*]}"
            docker rmi -f "$IMAGE_NAME" 2>/dev/null || true
            need_rebuild=true
        fi
    else
        need_rebuild=true
    fi

    # Build image if needed
    if [[ "$need_rebuild" == "true" ]] || ! docker image inspect "$IMAGE_NAME" >/dev/null 2>&1; then
        logo
        local build_context
        build_context=$(mktemp -d /tmp/claudebox-build.XXXXXX)
        local dockerfile="$build_context/Dockerfile"
        trap "rm -rf '$build_context'" EXIT

        # Create files locally first
        create_build_files "$build_context"

        cat > "$dockerfile" <<'DOCKERFILE'
FROM debian:bookworm
ARG USER_ID GROUP_ID USERNAME NODE_VERSION

RUN echo '#!/bin/sh\nexit 101' > /usr/sbin/policy-rc.d && chmod +x /usr/sbin/policy-rc.d

# Install locales first to fix locale warnings
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq locales && \
    echo "en_US.UTF-8 UTF-8" > /etc/locale.gen && \
    locale-gen en_US.UTF-8 && \
    rm -rf /var/lib/apt/lists/*

# Set locale environment variables
ENV LANG=en_US.UTF-8 \
    LANGUAGE=en_US:en \
    LC_ALL=en_US.UTF-8

RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq curl gnupg ca-certificates sudo git iptables ipset && \
    apt-get clean

RUN groupadd -g $GROUP_ID $USERNAME || true && \
    useradd -m -u $USER_ID -g $GROUP_ID -s /bin/bash $USERNAME

# Persist bash history
RUN SNIPPET="export PROMPT_COMMAND='history -a' && export HISTFILE=/commandhistory/.bash_history" \
    && mkdir /commandhistory \
    && touch /commandhistory/.bash_history \
    && chown -R $USERNAME /commandhistory

RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq \
    build-essential git wget curl unzip file vim nano \
    jq make less rsync openssh-client \
    procps sudo fzf zsh man-db gnupg2 \
    iptables ipset iproute2 dnsutils aggregate && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

RUN curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg && \
    chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg && \
    echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | tee /etc/apt/sources.list.d/github-cli.list > /dev/null && \
    apt update -qq && \
    apt install gh -y -qq && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

RUN DELTA_VERSION=$(curl -s https://api.github.com/repos/dandavison/delta/releases/latest | grep -Po '"tag_name": "\K[^"]*') && \
    ARCH=$(dpkg --print-architecture) && \
    wget -q https://github.com/dandavison/delta/releases/download/${DELTA_VERSION}/git-delta_${DELTA_VERSION}_${ARCH}.deb && \
    dpkg -i git-delta_${DELTA_VERSION}_${ARCH}.deb && \
    rm git-delta_${DELTA_VERSION}_${ARCH}.deb

USER $USERNAME
WORKDIR /home/$USERNAME

RUN sh -c "$(wget -O- https://github.com/deluan/zsh-in-docker/releases/download/v1.2.0/zsh-in-docker.sh)" -- \
    -p git \
    -p fzf \
    -a "source /usr/share/doc/fzf/examples/key-bindings.zsh" \
    -a "source /usr/share/doc/fzf/examples/completion.zsh" \
    -a "export PROMPT_COMMAND='history -a' && export HISTFILE=/commandhistory/.bash_history" \
    -a 'export HISTFILE=$HOME/.history/.zsh_history' \
    -a 'export HISTSIZE=10000' \
    -a 'export SAVEHIST=10000' \
    -a 'setopt HIST_IGNORE_DUPS' \
    -a 'setopt SHARE_HISTORY' \
    -a 'mkdir -p $HOME/.history' \
    -a 'export NVM_DIR="$HOME/.nvm"' \
    -a '[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"' \
    -a '[ -s "$NVM_DIR/bash_completion" ] && \. "$NVM_DIR/bash_completion"' \
    -x

RUN curl -LsSf https://astral.sh/uv/install.sh | sh
RUN echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc && \
    echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.zshrc

RUN git config --global core.pager delta && \
    git config --global interactive.diffFilter "delta --color-only" && \
    git config --global delta.navigate true && \
    git config --global delta.light false && \
    git config --global delta.side-by-side true

# Set DEVCONTAINER environment variable to help with orientation
ENV DEVCONTAINER=true

# Set the default shell to zsh rather than sh
ENV SHELL=/bin/zsh

ENV NVM_DIR="/home/$USERNAME/.nvm"
RUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash

RUN bash -c "source $NVM_DIR/nvm.sh && \
    if [[ \"$NODE_VERSION\" == '--lts' ]]; then \
        nvm install --lts && \
        nvm alias default 'lts/*'; \
    else \
        nvm install $NODE_VERSION && \
        nvm alias default $NODE_VERSION; \
    fi && \
    nvm use default"

RUN echo 'export NVM_DIR="$HOME/.nvm"' >> ~/.bashrc && \
    echo '[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"' >> ~/.bashrc && \
    echo '[ -s "$NVM_DIR/bash_completion" ] && \. "$NVM_DIR/bash_completion"' >> ~/.bashrc

RUN bash -c "source $NVM_DIR/nvm.sh && \
    nvm use default && \
    npm install -g @anthropic-ai/claude-code"

# Copy firewall script
COPY --chown=$USERNAME --chmod=755 init-firewall.sh /home/$USERNAME/init-firewall.sh

# Install profile packages as separate layers for better caching
USER root
DOCKERFILE

        # Add profile installations based on current profiles
        if [[ ${#current_profiles[@]} -gt 0 ]]; then
            info "Building with profiles: ${current_profiles[*]}"
            for profile in "${current_profiles[@]}"; do
                case "$profile" in
                    c)
                        cat >> "$dockerfile" <<'DOCKERFILE'
# C/C++ Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq build-essential gcc g++ gdb valgrind cmake ninja-build clang clang-format clang-tidy cppcheck doxygen libboost-all-dev autoconf automake libtool pkg-config libcmocka-dev libcmocka0 lcov libncurses5-dev libncursesw5-dev && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

DOCKERFILE
                        ;;
                    python)
    cat >> "$dockerfile" <<'DOCKERFILE'
# Python Development Profile (uv-managed Python only)
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq build-essential libffi-dev libssl-dev && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

DOCKERFILE
                        ;;
                    rust)
                        cat >> "$dockerfile" <<'DOCKERFILE'
# Rust Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq curl build-essential pkg-config libssl-dev && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

USER $USERNAME
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y && \
    echo 'source $HOME/.cargo/env' >> ~/.bashrc && \
    echo 'source $HOME/.cargo/env' >> ~/.zshrc
USER root

DOCKERFILE
                        ;;
                    go)
                        cat >> "$dockerfile" <<'DOCKERFILE'
# Go Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq wget git build-essential && \
    apt-get clean && rm -rf /var/lib/apt/lists/* && \
    GO_VERSION="1.21.5" && \
    wget -q "https://go.dev/dl/go${GO_VERSION}.linux-amd64.tar.gz" && \
    tar -C /usr/local -xzf "go${GO_VERSION}.linux-amd64.tar.gz" && \
    rm "go${GO_VERSION}.linux-amd64.tar.gz" && \
    echo 'export PATH=$PATH:/usr/local/go/bin' >> /etc/profile.d/go.sh

DOCKERFILE
                        ;;
                    javascript)
                        cat >> "$dockerfile" <<'DOCKERFILE'
# JavaScript Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq build-essential python3 && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

DOCKERFILE
                        ;;
                    java)
                        cat >> "$dockerfile" <<'DOCKERFILE'
# Java Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq openjdk-17-jdk maven gradle ant && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

DOCKERFILE
                        ;;
                    openwrt)
                        cat >> "$dockerfile" <<'DOCKERFILE'
# OpenWRT Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq build-essential gcc g++ make git wget unzip sudo file python3 python3-distutils rsync libncurses5-dev zlib1g-dev gawk gettext libssl-dev xsltproc libelf-dev libtool automake autoconf ccache subversion swig time qemu-system-arm qemu-system-aarch64 qemu-system-mips qemu-system-x86 qemu-utils && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

DOCKERFILE
                        ;;
                    ruby|php|database|devops|web|embedded|datascience|security|ml)
                        warn "Profile $profile not yet implemented in cached build"
                        ;;
                    *)
                        warn "Unknown profile: $profile"
                        ;;
                esac
            done
        fi

        # Add label with profile hash
        echo "# Label the image with the profile hash for change detection" >> "$dockerfile"
        echo "LABEL claudebox.profiles=\"$profile_hash\"" >> "$dockerfile"
        echo "" >> "$dockerfile"
        cat >> "$dockerfile" <<'DOCKERFILE'

USER $USERNAME

RUN bash -c "source $NVM_DIR/nvm.sh && claude --version"

# Copy Claude wrapper script
COPY --chown=$USERNAME --chmod=755 claude-wrapper /home/$USERNAME/claude-wrapper

WORKDIR /workspace

USER root
# Copy docker entrypoint script
COPY --chown=$USERNAME docker-entrypoint.sh /usr/local/bin/docker-entrypoint
RUN sed -i "s/DOCKERUSER/$USERNAME/g" /usr/local/bin/docker-entrypoint && \
    sed -i "s/DOCKERUSER/$USERNAME/g" /home/$USERNAME/init-firewall.sh && \
    chmod +x /usr/local/bin/docker-entrypoint

ENTRYPOINT ["/usr/local/bin/docker-entrypoint"]
DOCKERFILE

        run_docker_build "$dockerfile" "$build_context"

        echo -e "\n${GREEN}Complete!${NC}\n"
        success "Docker image '$IMAGE_NAME' built!"

        echo
        cecho "ClaudeBox Setup Complete!" "$CYAN"
        echo
        cecho "Quick Start:" "$GREEN"
        echo -e "  ${YELLOW}claudebox [options]${NC}        # Launch Claude CLI"
        echo
        cecho "Power Features:" "$GREEN"
        echo -e "  ${YELLOW}claudebox profile${NC}                # See all language profiles"
        echo -e "  ${YELLOW}claudebox profile c openwrt${NC}      # Install C + OpenWRT tools"
        echo -e "  ${YELLOW}claudebox profile python ml${NC}      # Install Python + ML stack"
        echo -e "  ${YELLOW}claudebox install <packages>${NC}     # Install additional apt packages"
        echo -e "  ${YELLOW}claudebox shell${NC}                  # Open bash shell in container"
        echo
        cecho "Security:" "$GREEN"
        echo -e "  Network firewall: ON by default (Anthropic recommended)"
        echo -e "  Sudo access: OFF by default"
        echo
        cecho "Maintenance:" "$GREEN"
        echo -e "  ${YELLOW}claudebox clean --help${NC}            # See all cleanup options"
        echo
        cecho "Just install the profile you need and start coding!" "$PURPLE"
       exit 0
   fi

   # Run container
   local extra_mounts=()

   # Ensure .claudebox exists with proper permissions
   if [[ ! -d "$HOME/.claudebox" ]]; then
       mkdir -p "$HOME/.claudebox"
   fi

   # Fix permissions if needed
   if [[ ! -w "$HOME/.claudebox" ]]; then
       warn "Fixing .claudebox permissions..."
       sudo chown -R "$USER:$USER" "$HOME/.claudebox" || true
   fi

   # Create project-specific allowlist if it doesn't exist
   local project_folder_name
   project_folder_name=$(get_project_folder_name "$PROJECT_DIR")
   local allowlist_file="$HOME/.claudebox/$project_folder_name/firewall/allowlist"

   if [[ ! -f "$allowlist_file" ]]; then
       mkdir -p "$(dirname "$allowlist_file")"
       info "Creating default firewall allowlist for project..."
       cat > "$allowlist_file" <<'EOF'
# ClaudeBox Firewall Allowlist
# Lines starting with # are comments
# Add one domain or IP range per line
#
# Default domains (always allowed):
# - api.anthropic.com
# - console.anthropic.com
# - statsig.anthropic.com
# - sentry.io

# ====================
# GitHub.com
# ====================
github.com
api.github.com
raw.githubusercontent.com
ssh.github.com
avatars.githubusercontent.com
codeload.github.com
objects.githubusercontent.com
pipelines.actions.githubusercontent.com
ghcr.io
pkg-containers.githubusercontent.com

# ====================
# GitLab.com
# ====================
gitlab.com
api.gitlab.com
registry.gitlab.com
uploads.gitlab.com
gitlab.io
*.gitlab.io
*.s3.amazonaws.com
*.amazonaws.com

# ====================
# Bitbucket.org
# ====================
bitbucket.org
api.bitbucket.org
altssh.bitbucket.org
bbuseruploads.s3.amazonaws.com
bitbucket-pipelines-prod-us-west-2.s3.amazonaws.com
bitbucket-pipelines-prod-us-east-1.s3.amazonaws.com
bitbucket-pipelines-prod-eu-west-1.s3.amazonaws.com

# ====================
# Atlassian IP Ranges (Bitbucket Cloud)
# ====================
104.192.136.0/21
185.166.140.0/22
13.200.41.128/25
18.246.31.128/25

# ====================
# Optional (Git LFS, Assets)
# ====================
github-cloud.s3.amazonaws.com
github-releases.githubusercontent.com
github-production-release-asset-2e65be.s3.amazonaws.com
EOF
       success "Created default allowlist"
   fi

   # Create new container with --rm

   # Check if we have a TTY
   local tty_flag=""
   # Get project-specific folder
   local project_folder_name
   project_folder_name=$(get_project_folder_name "$PROJECT_DIR")

   docker run -it --rm --init \
       -w /workspace \
       -v "$PROJECT_DIR":/workspace \
       -v "$CLAUDE_DATA_DIR":/home/$DOCKER_USER/.claude \
       -v "$HOME/.claudebox":/home/$DOCKER_USER/.claudebox \
       -v "$HOME/.config/claude":/home/$DOCKER_USER/.config/claude \
       -v "$HOME/.claude.json":/home/$DOCKER_USER/.claude.json \
       -v "$HOME/.npmrc":/home/$DOCKER_USER/.npmrc:ro \
       -v "$HOME/.ssh":/home/$DOCKER_USER/.ssh:ro \
       -e "NODE_ENV=${NODE_ENV:-production}" \
       -e "ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}" \
       --cap-add NET_ADMIN \
       --cap-add NET_RAW \
       "$IMAGE_NAME" "$@" --mcp-config /home/$DOCKER_USER/.claudebox/.mcp.json
}

main "$@"
