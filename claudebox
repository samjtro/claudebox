#!/usr/bin/env bash
set -euo pipefail

# Configuration
DEFAULT_FLAGS=()
readonly IMAGE_NAME="claudebox"
readonly DOCKER_USER="claude"
readonly USER_ID=$(id -u)
readonly GROUP_ID=$(id -g)
readonly PROJECT_DIR="$(pwd)"

# Cross-platform script path resolution
get_script_path() {
    local source="${BASH_SOURCE[0]:-$0}"
    while [[ -L "$source" ]]; do
        local dir="$(cd -P "$(dirname "$source")" && pwd)"
        source="$(readlink "$source")"
        [[ $source != /* ]] && source="$dir/$source"
    done
    echo "$(cd -P "$(dirname "$source")" && pwd)/$(basename "$source")"
}
readonly SCRIPT_PATH="$(get_script_path)"

readonly CLAUDE_DATA_DIR="$HOME/.claude"
readonly LINK_TARGET="$HOME/.local/bin/claudebox"
readonly NODE_VERSION="--lts"

# Default flags - comment/uncomment as needed
#DEFAULT_FLAGS+=("--dangerously-skip-permissions")
#DEFAULT_FLAGS+=("--dangerously-enable-sudo")
#DEFAULT_FLAGS+=("--dangerously-disable-firewall")

# Color codes
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly PURPLE='\033[0;35m'
readonly CYAN='\033[0;36m'
readonly WHITE='\033[1;37m'
readonly NC='\033[0m'

# Utility functions
cecho() { echo -e "${2:-$NC}$1${NC}"; }
error() { cecho "$1" "$RED" >&2; exit "${2:-1}"; }
warn() { cecho "$1" "$YELLOW"; }
info() { cecho "$1" "$BLUE"; }
success() { cecho "$1" "$GREEN"; }

# Parse early flags
VERBOSE=false
for arg in "$@"; do
    case "$arg" in
        -v|--verbose) VERBOSE=true ;;
    esac
done

# Logo
logo() {
    local cb='
 ██████╗██╗      █████╗ ██╗   ██╗██████╗ ███████╗
██╔════╝██║     ██╔══██╗██║   ██║██╔══██╗██╔════╝
██║     ██║     ███████║██║   ██║██║  ██║█████╗
██║     ██║     ██╔══██║██║   ██║██║  ██║██╔══╝
╚██████╗███████╗██║  ██║╚██████╔╝██████╔╝███████╗
 ╚═════╝╚══════╝╚═╝  ╚═╝ ╚═════╝ ╚═════╝ ╚══════╝

██████╗  ██████╗ ██╗  ██╗ ------ ┌──────────────┐
██╔══██╗██╔═══██╗╚██╗██╔╝ ------ │ The Ultimate │
██████╔╝██║   ██║ ╚███╔╝  ------ │ Claude Code  │
██╔══██╗██║   ██║ ██╔██╗  ------ │  Docker Dev  │
██████╔╝╚██████╔╝██╔╝ ██╗ ------ │ Environment  │
╚═════╝  ╚═════╝ ╚═╝  ╚═╝ ------ └──────────────┘
'
    while IFS= read -r l; do
        o="" c=""
        for ((i=0;i<${#l};i++)); do
            ch="${l:$i:1}"
            [[ "$ch" == " " ]] && { o+="$ch"; continue; }
            cc=$(printf '%d' "'$ch" 2>/dev/null||echo 0)
            if [[ $cc -ge 32 && $cc -le 126 ]]; then n='\033[33m'
            elif [[ $cc -ge 9552 && $cc -le 9580 ]]; then n='\033[34m'
            elif [[ $cc -eq 9608 ]]; then n='\033[31m'
            else n='\033[37m'; fi
            [[ "$n" != "$c" ]] && { o+="$n"; c="$n"; }
            o+="$ch"
        done
        echo -e "${o}\033[0m"
    done <<< "$cb"
}

# Symlink management
update_symlink() {
    if [[ -L "$LINK_TARGET" ]]; then
        local current_target
        current_target=$(readlink "$LINK_TARGET" 2>/dev/null || echo "")
        if [[ "$current_target" != "$SCRIPT_PATH" ]]; then
            rm -f "$LINK_TARGET"
            ln -s "$SCRIPT_PATH" "$LINK_TARGET"
            info "Updated claudebox symlink to current location"
        fi
    elif [[ ! -e "$LINK_TARGET" ]]; then
        mkdir -p "$(dirname "$LINK_TARGET")"
        if ln -s "$SCRIPT_PATH" "$LINK_TARGET" 2>/dev/null; then
            success "Created claudebox symlink in $(dirname "$LINK_TARGET")"
        else
            warn "Note: Could not create symlink (needs sudo)"
            warn "Run with sudo or add $(dirname "$LINK_TARGET") to your PATH"
        fi
    fi
}

# Docker checks
check_docker() {
    command -v docker &>/dev/null || return 1
    docker info &>/dev/null || return 2
    docker ps &>/dev/null || return 3
    return 0
}

install_docker() {
    warn "Docker is not installed."
    cecho "Would you like to install Docker now? (y/n)" "$CYAN"
    read -r response
    [[ "$response" =~ ^[Yy]$ ]] || error "Docker is required. Visit: https://docs.docker.com/engine/install/"

    info "Installing Docker..."

    [[ -f /etc/os-release ]] && . /etc/os-release || error "Cannot detect OS"

    case "${ID:-}" in
        ubuntu|debian)
            warn "Installing Docker requires sudo privileges..."
            sudo apt-get update
            sudo apt-get install -y ca-certificates curl gnupg lsb-release
            sudo mkdir -p /etc/apt/keyrings
            curl -fsSL "https://download.docker.com/linux/$ID/gpg" | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
            echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/$ID $(lsb_release -cs) stable" | \
                sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
            sudo apt-get update
            sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
            ;;
        fedora|rhel|centos)
            warn "Installing Docker requires sudo privileges..."
            sudo dnf -y install dnf-plugins-core
            sudo dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo
            sudo dnf install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
            sudo systemctl start docker
            sudo systemctl enable docker
            ;;
        arch|manjaro)
            warn "Installing Docker requires sudo privileges..."
            sudo pacman -S --noconfirm docker
            sudo systemctl start docker
            sudo systemctl enable docker
            ;;
        *)
            error "Unsupported OS: ${ID:-unknown}. Visit: https://docs.docker.com/engine/install/"
            ;;
    esac

    success "Docker installed successfully!"
    configure_docker_nonroot
}

configure_docker_nonroot() {
    warn "Configuring Docker for non-root usage..."
    warn "This requires sudo to add you to the docker group..."

    getent group docker >/dev/null || sudo groupadd docker
    sudo usermod -aG docker "$USER"

    success "Docker configured for non-root usage!"
    warn "You need to log out and back in for group changes to take effect."
    warn "Or run: ${CYAN}newgrp docker"
    warn "Then run 'claudebox' again."
    info "Trying to activate docker group in current shell..."
    exec newgrp docker
}

# Spinner
show_spinner() {
    local pid=$1 msg=$2 spin='⠋⠙⠹⠸⠼⠴⠦⠧⠇⠏' i=0
    echo -n "$msg "
    while kill -0 "$pid" 2>/dev/null; do
        printf "\b%s" "${spin:i++%${#spin}:1}"
        sleep 0.1
    done
    echo -e "\b${GREEN}✓${NC}"
}

# Profile definitions (Bash 3.2 compatible)
# Using function-based approach instead of associative arrays for compatibility
get_profile_packages() {
    local profile="$1"
    case "$profile" in
        c) echo "build-essential gcc g++ gdb valgrind cmake ninja-build clang clang-format clang-tidy cppcheck doxygen libboost-all-dev autoconf automake libtool pkg-config libcmocka-dev libcmocka0 lcov libncurses5-dev libncursesw5-dev";;
        openwrt) echo "build-essential gcc g++ make git wget unzip sudo file python3 python3-distutils rsync libncurses5-dev zlib1g-dev gawk gettext libssl-dev xsltproc libelf-dev libtool automake autoconf ccache subversion swig time qemu-system-arm qemu-system-aarch64 qemu-system-mips qemu-system-x86 qemu-utils";;
        rust) echo "curl build-essential pkg-config libssl-dev";;
        python) echo "python3 python3-pip python3-venv python3-dev build-essential libffi-dev libssl-dev";;
        go) echo "wget git build-essential";;
        javascript) echo "build-essential python3";;
        java) echo "openjdk-17-jdk maven gradle ant";;
        ruby) echo "ruby-full ruby-dev build-essential zlib1g-dev libssl-dev libreadline-dev libyaml-dev libsqlite3-dev sqlite3 libxml2-dev libxslt1-dev libcurl4-openssl-dev software-properties-common libffi-dev";;
        php) echo "php php-cli php-fpm php-mysql php-pgsql php-sqlite3 php-curl php-gd php-mbstring php-xml php-zip composer";;
        database) echo "postgresql-client mysql-client sqlite3 redis-tools mongodb-clients";;
        devops) echo "docker.io docker-compose kubectl helm terraform ansible awscli";;
        web) echo "nginx apache2-utils curl wget httpie jq";;
        embedded) echo "gcc-arm-none-eabi gdb-multiarch openocd picocom minicom screen platformio";;
        datascience) echo "python3-numpy python3-pandas python3-scipy python3-matplotlib python3-seaborn jupyter-notebook r-base";;
        security) echo "nmap tcpdump wireshark-common netcat-openbsd john hashcat hydra";;
        ml) echo "python3-pip python3-dev python3-venv build-essential cmake";;
        *) echo "";;
    esac
}

get_profile_description() {
    local profile="$1"
    case "$profile" in
        c) echo "C/C++ Development (compilers, debuggers, analyzers, build tools, cmocka, coverage, ncurses)";;
        openwrt) echo "OpenWRT Development (cross-compilation, QEMU, build essentials)";;
        rust) echo "Rust Development (cargo and rustc will be installed separately)";;
        python) echo "Python Development (Python 3, pip, venv, build tools)";;
        go) echo "Go Development (Go will be installed separately)";;
        javascript) echo "JavaScript/TypeScript Development (Node.js, npm, yarn)";;
        java) echo "Java Development (OpenJDK 17, Maven, Gradle, Ant)";;
        ruby) echo "Ruby Development (Ruby, gems, build tools)";;
        php) echo "PHP Development (PHP, Composer, common extensions)";;
        database) echo "Database Tools (PostgreSQL, MySQL, SQLite, Redis, MongoDB clients)";;
        devops) echo "DevOps Tools (Docker, K8s, Terraform, Ansible, AWS CLI)";;
        web) echo "Web Development Tools (nginx, curl, httpie, jq)";;
        embedded) echo "Embedded Development (ARM toolchain, debuggers, serial tools)";;
        datascience) echo "Data Science (NumPy, Pandas, Jupyter, R)";;
        security) echo "Security Tools (network analysis, penetration testing)";;
        ml) echo "Machine Learning (base tools, Python libs installed separately)";;
        *) echo "Unknown profile";;
    esac
}

# List of all available profiles
AVAILABLE_PROFILES="c openwrt rust python go javascript java ruby php database devops web embedded datascience security ml"

# Docker operations
docker_exec_root() {
    docker exec -u root "$@"
}

docker_exec_user() {
    docker exec -u "$DOCKER_USER" "$@"
}

# Helper functions for project profiles
get_project_folder_name() {
    echo "$1" | sed 's|^/||; s|/|-|g'
}

get_profile_file_path() {
    local profile_dir="$HOME/.claudebox/profiles"
    mkdir -p "$profile_dir"
    echo "$profile_dir/$(echo "$PROJECT_DIR" | sed 's|/|-|g' | sed 's|^-||').ini"
}

read_profile_section() {
    local profile_file="$1"
    local section="$2"
    local result=()

    if [[ -f "$profile_file" ]] && grep -q "^\[$section\]" "$profile_file"; then
        while IFS= read -r line; do
            [[ -z "$line" || "$line" =~ ^\[.*\]$ ]] && break
            result+=("$line")
        done < <(sed -n "/^\[$section\]/,/^\[/p" "$profile_file" | tail -n +2 | grep -v '^\[')
    fi

    printf '%s\n' "${result[@]}"
}

update_profile_section() {
    local profile_file="$1"
    local section="$2"
    shift 2
    local new_items=("$@")

    # Read existing items
    local existing_items=()
    readarray -t existing_items < <(read_profile_section "$profile_file" "$section")

    # Merge with new items (avoid duplicates)
    local all_items=()
    for item in "${existing_items[@]}"; do
        [[ -n "$item" ]] && all_items+=("$item")
    done

    for item in "${new_items[@]}"; do
        local found=false
        for existing in "${all_items[@]}"; do
            [[ "$existing" == "$item" ]] && found=true && break
        done
        [[ "$found" == "false" ]] && all_items+=("$item")
    done

    # Write updated profile file
    {
        # Preserve other sections
        if [[ -f "$profile_file" ]]; then
            awk -v sect="$section" '
                BEGIN { in_section=0; skip_section=0 }
                /^\[/ {
                    if ($0 == "[" sect "]") { skip_section=1; in_section=1 }
                    else { skip_section=0; in_section=0 }
                }
                !skip_section { print }
                /^\[/ && !skip_section && in_section { in_section=0 }
            ' "$profile_file"
        fi

        # Add our section
        echo "[$section]"
        for item in "${all_items[@]}"; do
            echo "$item"
        done
        echo ""
    } > "${profile_file}.tmp" && mv "${profile_file}.tmp" "$profile_file"
}

# Project-specific folder setup
setup_project_folder() {
    local project_folder_name
    project_folder_name=$(get_project_folder_name "$PROJECT_DIR")
    PROJECT_CLAUDEBOX_DIR="$HOME/.claudebox/$project_folder_name"

    mkdir -p "$PROJECT_CLAUDEBOX_DIR/claude-config"
    mkdir -p "$PROJECT_CLAUDEBOX_DIR/memory"
    mkdir -p "$PROJECT_CLAUDEBOX_DIR/context"
    mkdir -p "$PROJECT_CLAUDEBOX_DIR/firewall"

    info "Created project folder: $PROJECT_CLAUDEBOX_DIR"
}

# Get the script directory
get_script_dir() {
    local source="${BASH_SOURCE[0]}"
    while [ -h "$source" ]; do
        local dir="$(cd -P "$(dirname "$source")" && pwd)"
        source="$(readlink "$source")"
        [[ $source != /* ]] && source="$dir/$source"
    done
    echo "$(cd -P "$(dirname "$source")" && pwd)"
}

SCRIPT_DIR="$(get_script_dir)"

# Global MCP configuration setup
setup_global_mcp_config() {
    local mcp_file="$HOME/.claudebox/.mcp.json"
    local default_config="$SCRIPT_DIR/lib/mcp/default-config.json"
    
    # Create directory if needed
    mkdir -p "$(dirname "$mcp_file")"
    
    # If config doesn't exist or is outdated, copy from default
    if [[ ! -f "$mcp_file" ]] || [[ -f "$default_config" && "$default_config" -nt "$mcp_file" ]]; then
        if [[ -f "$default_config" ]]; then
            cp "$default_config" "$mcp_file"
            info "Updated global MCP config from template"
        else
            # Fallback to embedded config if lib file doesn't exist
            cat > "$mcp_file" << 'EOF'
{
  "mcpServers": {
    "memory": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-memory"]
    },
    "sequential-thinking": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-sequential-thinking"]
    },
    "context7": {
      "command": "npx",
      "args": ["-y", "@upstash/context7-mcp"]
    },
    "openrouterai": {
      "command": "npx",
      "args": ["-y", "@mcpservers/openrouterai"],
      "env": {
        "OPENROUTER_API_KEY": "${OPENROUTER_API_KEY:-}",
        "OPENROUTER_DEFAULT_MODEL": "${OPENROUTER_DEFAULT_MODEL:-}"
      }
    }
  }
}
EOF
            info "Created global MCP config"
        fi
    fi
}

# Setup Claude agent command
setup_claude_agent_command() {
    mkdir -p .claude/commands
    cat << 'EOF' > .claude/commands/agent.md
# Agentic Loop Framework
<System>
You are building an Agentic Loop that can tackle any complex task with minimal role bloat.
Core principles
1. Single-brain overview – One Orchestrator owns the big picture.
2. Few, powerful agents – Reuse the same Specialist prompt for parallelism instead of inventing many micro-roles.
3. Tight feedback – A dedicated Evaluator grades outputs (0-100) and suggests concrete fixes until quality ≥ TARGET_SCORE.
4. Shared context – Every agent receives the same `context.md` so no information is siloed.
5. Repo-aware – The Orchestrator decides whether to align to the current repo or create a generic loop.
6. Explicit imperatives – Use the labels "You Must" or "Important" for non-negotiable steps; permit extra compute with "Think hard" / "ultrathink".
</System>
<Context>
Task: <<USER_DESCRIBED_TASK>>
Repo path (if any): <<ABSOLUTE_PATH_OR_NONE>>
Desired parallelism: <<N_PARALLEL_SPECIALISTS>>  (1-3 is typical)
The Orchestrator must decide:
- Whether to specialise the workflow to this repo or keep it generic.
- How many identical Specialist instances to launch (0 = sequential).
</Context>
<Instructions>
### 0  Bootstrap
- You Must create `./docs/<TASK>/context.md` containing this entire block so all agents share it.
### 1  Orchestrator
EOF
    info "Created Claude agent command template"
}

# Setup APM (Agentic Project Management) commands
setup_apm_commands() {
    local commands_dir=".claude/commands"
    local lib_dir="$SCRIPT_DIR/lib/apm/commands"
    
    mkdir -p "$commands_dir"
    
    # Copy APM commands from lib if available
    if [[ -d "$lib_dir" ]]; then
        # Only copy if files don't exist or lib is newer
        for cmd_file in "$lib_dir"/*.md; do
            [[ -f "$cmd_file" ]] || continue
            local basename="$(basename "$cmd_file")"
            local target="$commands_dir/$basename"
            
            if [[ ! -f "$target" ]] || [[ "$cmd_file" -nt "$target" ]]; then
                cp "$cmd_file" "$target"
            fi
        done
        info "APM commands synchronized"
    else
        # Fallback to embedded commands if lib doesn't exist
        cat << 'EOF' > .claude/commands/apm-manager.md
# APM Manager Agent Initialization

You are now the Manager Agent for this project. Your role is to:
1. Create and maintain the Implementation Plan
2. Assign tasks to Implementation Agents
3. Review work and update the Memory Bank
4. Coordinate handovers when context limits are reached

## Initial Setup Tasks:
1. Review the codebase and understand the project structure
2. Create the initial Implementation Plan in `IMPLEMENTATION_PLAN.md`
3. Set up the Memory Bank in `MEMORY_BANK.md`
4. Begin task assignment for the current objectives

## Key Responsibilities:
- Break down complex objectives into manageable tasks
- Create clear, actionable task prompts for Implementation Agents
- Maintain accurate records in the Memory Bank
- Review completed work and provide feedback
- Manage agent handovers when needed

Refer to the APM documentation for detailed workflows and formats.
EOF

    # Create apm-implement command
    cat << 'EOF' > .claude/commands/apm-implement.md
# APM Implementation Agent Onboarding

You are an Implementation Agent. Your role is to:
1. Execute the specific task assigned by the Manager Agent
2. Follow the implementation guidelines and standards
3. Report progress and results clearly
4. Update relevant logs for the Memory Bank

## Key Guidelines:
- Focus only on your assigned task
- Follow existing code patterns and conventions
- Test your implementation thoroughly
- Document any assumptions or decisions made
- Report blockers or issues immediately

## Workflow:
1. Understand your assigned task completely
2. Plan your approach
3. Implement the solution
4. Test and verify
5. Report results back to the user

Remember: Quality over speed. Ensure your work is production-ready.
EOF

    # Create apm-task command
    cat << 'EOF' > .claude/commands/apm-task.md
# Generate APM Task Assignment

Create a task assignment prompt following the APM format:

## Task Assignment Template:
- Task ID: [Generate unique ID]
- Priority: [High/Medium/Low]
- Estimated Effort: [Hours/Days]
- Dependencies: [List any dependencies]

## Task Description:
[Clear, specific description of what needs to be done]

## Success Criteria:
[Measurable outcomes that define task completion]

## Technical Context:
[Relevant technical details, files, APIs, etc.]

## Constraints:
[Any limitations or requirements to consider]

Generate the task assignment based on the current Implementation Plan and project needs.
EOF

    # Create apm-memory command
    cat << 'EOF' > .claude/commands/apm-memory.md
# APM Memory Bank Management

## Memory Bank Operations:
1. **Initialize**: Create new Memory Bank structure
2. **Update**: Add new log entries
3. **Query**: Search and retrieve information
4. **Archive**: Move old entries to archive

## Log Entry Format:
```
[TIMESTAMP] [AGENT_TYPE] [TASK_ID]
Action: [What was done]
Result: [Outcome/Status]
Notes: [Additional context]
---
```

## Commands:
- To add entry: Append to MEMORY_BANK.md following the format
- To search: Use keyword search across all entries
- To summarize: Create executive summary of recent activities

Maintain chronological order and ensure all significant actions are logged.
EOF

    # Create apm-handover command
    cat << 'EOF' > .claude/commands/apm-handover.md
# APM Handover Protocol

## Handover Checklist:
1. **Current State Summary**: Document what has been completed
2. **Pending Tasks**: List unfinished work with current progress
3. **Key Decisions**: Important choices made and their rationale
4. **Known Issues**: Any blockers or problems encountered
5. **Next Steps**: Clear guidance for the next agent

## Handover Artifact Format:
```
HANDOVER ARTIFACT
Generated: [TIMESTAMP]
From: [Current Agent ID]
To: [Next Agent ID]

## Completed Work:
[List of completed tasks/features]

## Current Task Progress:
[Status of in-progress work]

## Critical Context:
[Essential information for continuity]

## Recommended Actions:
[Specific next steps]
```

Execute handover when approaching context limits or role transition.
EOF

    # Create apm-plan command
    cat << 'EOF' > .claude/commands/apm-plan.md
# APM Implementation Plan Management

## Plan Structure:
1. **Project Overview**: High-level objectives and scope
2. **Phase Breakdown**: Major phases with milestones
3. **Task Inventory**: Detailed task list with dependencies
4. **Resource Allocation**: Agent assignments and timelines
5. **Risk Assessment**: Potential issues and mitigation

## Plan Operations:
- **Create**: Initialize new Implementation Plan
- **Update**: Modify tasks, phases, or timelines
- **Track**: Update task statuses and progress
- **Report**: Generate progress summaries

## Format Example:
```
# Implementation Plan

## Phase 1: [Name]
- Milestone: [Description]
- Timeline: [Duration]
- Tasks:
  - [ ] Task 1.1: [Description] (ID: T001)
  - [ ] Task 1.2: [Description] (ID: T002)
```

Maintain plan accuracy and update after each task completion.
EOF

    # Create apm-review command
    cat << 'EOF' > .claude/commands/apm-review.md
# APM Work Review Protocol

## Review Checklist:
1. **Completeness**: Does the work fulfill all requirements?
2. **Quality**: Does it meet coding standards and best practices?
3. **Testing**: Has it been properly tested?
4. **Documentation**: Is it adequately documented?
5. **Integration**: Does it integrate well with existing code?

## Review Process:
1. Examine the completed work against task requirements
2. Run tests and verify functionality
3. Check code quality and standards compliance
4. Assess impact on other system components
5. Provide constructive feedback

## Feedback Format:
```
## Review for Task [ID]
**Status**: [Approved/Needs Revision]
**Quality Score**: [1-10]

### Strengths:
- [Positive aspects]

### Areas for Improvement:
- [Specific issues to address]

### Recommended Actions:
- [Next steps]
```

Be thorough but constructive in reviews.
EOF

    info "Created APM command templates"
    fi
}

# Setup APM documentation
setup_apm_docs() {
    local docs_dir=".claude/apm-docs"
    local lib_docs="$SCRIPT_DIR/lib/apm/docs"
    
    mkdir -p "$docs_dir"
    
    # Copy documentation from lib if available
    if [[ -d "$lib_docs" ]]; then
        for doc_file in "$lib_docs"/*.md; do
            [[ -f "$doc_file" ]] || continue
            local basename="$(basename "$doc_file")"
            local target="$docs_dir/$basename"
            
            if [[ ! -f "$target" ]] || [[ "$doc_file" -nt "$target" ]]; then
                cp "$doc_file" "$target"
            fi
        done
        info "APM documentation synchronized"
    else
        # Fallback to embedded docs
        cat << 'EOF' > "$docs_dir/README.md"
# Agentic Project Management (APM) Framework

## Overview
APM is a structured framework for managing complex AI-assisted projects using multiple specialized agents. It provides clear workflows, role definitions, and communication protocols.

## Core Components

### 1. Manager Agent
- Creates and maintains Implementation Plans
- Assigns tasks to Implementation Agents
- Reviews completed work
- Manages the Memory Bank
- Coordinates agent handovers

### 2. Implementation Agents
- Execute specific tasks from the Implementation Plan
- Follow coding standards and project conventions
- Report progress and blockers
- Create logs for Memory Bank updates

### 3. Memory Bank
- Centralized log of all agent activities
- Searchable knowledge base
- Preserves context across agent instances
- Critical for handovers and reviews

### 4. Implementation Plan
- Detailed breakdown of project phases and tasks
- Task dependencies and priorities
- Resource allocation and timelines
- Progress tracking

## Workflow

1. **Manager Setup**: Initialize Manager Agent with project context
2. **Planning**: Manager creates Implementation Plan
3. **Task Assignment**: Manager prepares task prompts
4. **Execution**: Implementation Agents work on tasks
5. **Reporting**: Agents log work in Memory Bank format
6. **Review**: Manager reviews and updates plan
7. **Iteration**: Repeat until project completion

## Commands

- `/apm-manager` - Initialize Manager Agent
- `/apm-implement` - Create Implementation Agent
- `/apm-task` - Generate task assignment
- `/apm-memory` - Manage Memory Bank
- `/apm-handover` - Execute handover protocol
- `/apm-plan` - Manage Implementation Plan
- `/apm-review` - Review completed work

## Best Practices

1. **Clear Communication**: Use structured formats for all exchanges
2. **Regular Updates**: Keep Memory Bank current
3. **Context Preservation**: Document decisions and rationale
4. **Quality Focus**: Ensure production-ready implementations
5. **Proactive Handovers**: Plan for context limits

## Getting Started

1. Use `/apm-manager` to initialize the Manager Agent
2. Let Manager create initial Implementation Plan
3. Use `/apm-implement` when starting implementation work
4. Follow the structured workflow for best results

For detailed information about each component, refer to the individual command documentation.
EOF

        info "Created APM documentation"
    fi
}

# Setup Multi-Agent Framework
setup_agent_framework() {
    local agents_cmd=".claude/commands/apm-agents.md"
    local agents_lib="$SCRIPT_DIR/lib/agents/claudebox-agents"
    
    # Ensure directories exist
    mkdir -p .claude/bin
    
    # Copy the agents command if it exists in lib
    if [[ -f "$SCRIPT_DIR/lib/apm/commands/apm-agents.md" ]]; then
        cp "$SCRIPT_DIR/lib/apm/commands/apm-agents.md" "$agents_cmd"
    fi
    
    # Ensure the agents executable is available
    if [[ -f "$agents_lib" ]]; then
        # Create a wrapper script that can be executed from within the container
        cat << 'EOF' > .claude/bin/apm-agents
#!/bin/bash
# Wrapper for the multi-agent framework

# Source the actual implementation
exec /workspace/lib/agents/claudebox-agents "$@"
EOF
        chmod +x .claude/bin/apm-agents
        
        info "Multi-agent framework initialized"
    else
        warn "Multi-agent framework not found in lib/agents/"
    fi
}

run_docker_build() {
    info "Running docker build..."
    docker build \
        --build-arg USER_ID="$USER_ID" \
        --build-arg GROUP_ID="$GROUP_ID" \
        --build-arg USERNAME="$DOCKER_USER" \
        --build-arg NODE_VERSION="$NODE_VERSION" \
        -f "$1" -t "$IMAGE_NAME" .
}

# Main execution
main() {
    update_symlink

    # Check Docker
    local docker_status
    docker_status=$(check_docker; echo $?)
    case $docker_status in
        1) install_docker ;;
        2)
            warn "Docker is installed but not running."
            warn "Starting Docker requires sudo privileges..."
            sudo systemctl start docker
            docker info &>/dev/null || error "Failed to start Docker"
            docker ps &>/dev/null || configure_docker_nonroot
            ;;
        3)
            warn "Docker requires sudo. Setting up non-root access..."
            configure_docker_nonroot
            ;;
    esac

    # Handle commands
    [[ "$VERBOSE" == "true" ]] && echo "Command: ${1:-none}" >&2
    case "${1:-}" in
        profile)
            shift
            if [[ $# -eq 0 ]]; then
                cecho "Available Profiles:" "$CYAN"
                echo
                for profile in $AVAILABLE_PROFILES; do
                    local desc=$(get_profile_description "$profile")
                    echo -e "  ${GREEN}$profile${NC} - $desc"
                done
                echo
                warn "Usage: claudebox profile <name> [<name2> ...]"
                warn "Example: claudebox profile c python web"
                exit 0
            fi

            local selected=() remaining=()
            while [[ $# -gt 0 ]]; do
                local packages=$(get_profile_packages "$1")
                if [[ -n "$packages" ]]; then
                    selected+=("$1")
                    shift
                else
                    remaining=("$@")
                    break
                fi
            done

            [[ ${#selected[@]} -eq 0 ]] && error "No valid profiles specified\nRun 'claudebox profile' to see available profiles"

            local profile_file
            profile_file=$(get_profile_file_path)

            # Update profiles section
            update_profile_section "$profile_file" "profiles" "${selected[@]}"

            # Read all current profiles for display
            local all_profiles=()
            readarray -t all_profiles < <(read_profile_section "$profile_file" "profiles")

            cecho "Profile: $PROJECT_DIR" "$CYAN"
            cecho "Installing profiles: ${selected[*]}" "$PURPLE"
            if [[ ${#all_profiles[@]} -gt 0 ]]; then
                cecho "All active profiles: ${all_profiles[*]}" "$GREEN"
            fi
            echo

            # Continue to main execution to create container with profiles
            if [[ ${#remaining[@]} -gt 0 ]]; then
                set -- "${remaining[@]}"
            fi
            ;;

        install)
            shift
            [[ $# -eq 0 ]] && error "No packages specified. Usage: claudebox install <package1> <package2> ..."

            local profile_file
            profile_file=$(get_profile_file_path)

            # Update packages section
            update_profile_section "$profile_file" "packages" "$@"

            # Read all current packages for display
            local all_packages=()
            readarray -t all_packages < <(read_profile_section "$profile_file" "packages")

            cecho "Profile: $PROJECT_DIR" "$CYAN"
            cecho "Installing packages: $*" "$PURPLE"
            if [[ ${#all_packages[@]} -gt 0 ]]; then
                cecho "All packages: ${all_packages[*]}" "$GREEN"
            fi
            echo
            ;;

        save|commit)
            shift
            warn "The 'save' command is deprecated!"
            info "With --rm containers, there's nothing to save."
            info "Profiles and packages are tracked in profiles.json automatically."
            echo

            # Show current project's profile info
            local profile_file
            profile_file=$(get_profile_file_path)

            if [[ -f "$profile_file" ]]; then
                cecho "Current project profile info ($PROJECT_DIR):" "$CYAN"
                cat "$profile_file"
            else
                info "No profiles configured for current project."
            fi

            exit 0
            ;;

        update|config|mcp|migrate-installer)
            [[ ! -f /.dockerenv ]] && docker image inspect "$IMAGE_NAME" &>/dev/null || \
                error "ClaudeBox image not found.\nRun ${GREEN}claudebox${NC} first to build the image."

            local cmd=$1
            shift

            info "Running $cmd command..."

            # Create temporary container
            local temp_container
            temp_container=$(docker create "$IMAGE_NAME" sleep infinity)
            docker start "$temp_container" >/dev/null

            if [[ "$cmd" == "update" ]]; then
                info "Updating Claude code..."
                docker exec -u "$DOCKER_USER" "$temp_container" bash -c "
                    source \$HOME/.nvm/nvm.sh
                    nvm use default
                    claude update
                    claude --version
                "
            else
                docker exec -it -u "$DOCKER_USER" "$temp_container" \
                    /home/$DOCKER_USER/claude-wrapper "${DEFAULT_FLAGS[@]}" "$cmd" "$@"
            fi

            # Commit changes back to image
            docker commit "$temp_container" "$IMAGE_NAME" >/dev/null
            docker rm -f "$temp_container" >/dev/null

            success "$cmd completed and saved to image!"
            exit 0
            ;;

        shell)
            shift
            # Just add --shell-mode to the arguments and continue
            set -- "--shell-mode" "$@"
            ;;

        clean)
            shift
            case "${1:-}" in
                --help|-h)
                    cecho "ClaudeBox Clean Options:" "$CYAN"
                    echo
                    echo -e "  ${GREEN}clean${NC}                    Remove all containers (preserves image)"
                    echo -e "  ${GREEN}clean --project${NC}          Remove current project's data and profile"
                    echo -e "  ${GREEN}clean --all${NC}              Remove everything: containers, image, cache, symlink"
                    echo -e "  ${GREEN}clean --image${NC}            Remove containers and image (preserves build cache)"
                    echo -e "  ${GREEN}clean --cache${NC}            Remove Docker build cache only"
                    echo -e "  ${GREEN}clean --volumes${NC}          Remove associated Docker volumes"
                    echo -e "  ${GREEN}clean --symlink${NC}          Remove claudebox symlink only"
                    echo -e "  ${GREEN}clean --dangling${NC}         Remove dangling images and unused containers"
                    echo -e "  ${GREEN}clean --logs${NC}             Clear Docker container logs"
                    echo -e "  ${GREEN}clean --help${NC}             Show this help message"
                    echo
                    cecho "Examples:" "$YELLOW"
                    echo "  claudebox clean              # Remove all containers"
                    echo "  claudebox clean --project    # Remove only this project's container"
                    echo "  claudebox clean --image      # Remove containers and image"
                    echo "  claudebox clean --all        # Complete cleanup and reset"
                    exit 0
                    ;;
                --all|-a)
                    warn "Complete ClaudeBox cleanup: removing containers, image, cache, and symlink..."
                    # Remove all profile files
                    rm -rf "$HOME/.claudebox/profiles"
                    # Remove any containers from this image
                    docker ps -a --filter "ancestor=$IMAGE_NAME" -q | xargs -r docker rm -f 2>/dev/null || true
                    # Remove orphaned containers from images that no longer exist
                    # This is safer as it only removes containers whose images are gone
                    docker ps -a --filter "status=exited" --format "{{.ID}} {{.Image}}" | while read id image; do
                        if ! docker image inspect "$image" >/dev/null 2>&1; then
                            docker rm -f "$id" 2>/dev/null || true
                        fi
                    done
                    # Remove image
                    docker rmi -f "$IMAGE_NAME" 2>/dev/null || true
                    # Prune build cache
                    docker builder prune -af 2>/dev/null || true
                    # Remove volumes
                    docker volume ls -q --filter "name=claudebox" | xargs -r docker volume rm 2>/dev/null || true
                    # Remove symlink
                    [[ -L "$LINK_TARGET" ]] && rm -f "$LINK_TARGET" && info "Removed claudebox symlink"
                    success "Complete cleanup finished!"
                    ;;
                --image|-i)
                    warn "Removing ClaudeBox containers and image..."
                    # Remove all profile files
                    rm -rf "$HOME/.claudebox/profiles"
                    # Remove any containers from this image
                    docker ps -a --filter "ancestor=$IMAGE_NAME" -q | xargs -r docker rm -f 2>/dev/null || true
                    # Remove orphaned containers from images that no longer exist
                    # This is safer as it only removes containers whose images are gone
                    docker ps -a --filter "status=exited" --format "{{.ID}} {{.Image}}" | while read id image; do
                        if ! docker image inspect "$image" >/dev/null 2>&1; then
                            docker rm -f "$id" 2>/dev/null || true
                        fi
                    done
                    docker rmi -f "$IMAGE_NAME" 2>/dev/null || true
                    success "Containers and image removed! Build cache preserved."
                    ;;
                --cache|-c)
                    warn "Cleaning Docker build cache..."
                    docker builder prune -af
                    success "Build cache cleaned!"
                    ;;
                --volumes|-v)
                    warn "Removing ClaudeBox-related volumes..."
                    docker volume ls -q --filter "name=claudebox" | xargs -r docker volume rm 2>/dev/null || true
                    docker volume prune -f 2>/dev/null || true
                    success "Volumes cleaned!"
                    ;;
                --symlink|-s)
                    if [[ -L "$LINK_TARGET" ]]; then
                        rm -f "$LINK_TARGET"
                        success "Removed claudebox symlink from $(dirname "$LINK_TARGET")"
                    else
                        info "No claudebox symlink found at $LINK_TARGET"
                    fi
                    exit 0
                    ;;
                --dangling|-d)
                    warn "Removing dangling images and unused containers..."
                    docker image prune -f
                    docker container prune -f
                    success "Dangling resources cleaned!"
                    ;;
                --logs|-l)
                    warn "Clearing Docker container logs..."
                    docker ps -a --filter "ancestor=$IMAGE_NAME" -q | while read -r container; do
                        docker logs "$container" >/dev/null 2>&1 && echo -n | docker logs "$container" 2>/dev/null || true
                    done
                    success "Container logs cleared!"
                    ;;
                --project|-p)
                    warn "Removing data for current project: $PROJECT_DIR"
                    local profile_file
                    profile_file=$(get_profile_file_path)

                    # Remove profile
                    if [[ -f "$profile_file" ]]; then
                        rm -f "$profile_file"
                        success "Removed profile for $PROJECT_DIR"
                    fi

                    # Remove project-specific folder
                    local project_folder_name
                    project_folder_name=$(get_project_folder_name "$PROJECT_DIR")
                    local project_claudebox_dir="$HOME/.claudebox/$project_folder_name"

                    if [[ -d "$project_claudebox_dir" ]]; then
                        rm -rf "$project_claudebox_dir"
                        success "Removed project data folder: $project_claudebox_dir"
                    else
                        info "No project data folder found"
                    fi
                    exit 0
                    ;;
                *)
                    warn "Cleaning ClaudeBox containers..."
                    # Remove all profile files
                    rm -rf "$HOME/.claudebox/profiles"
                    # Remove any containers from this image
                    docker ps -a --filter "ancestor=$IMAGE_NAME" -q | xargs -r docker rm -f 2>/dev/null || true
                    # Remove orphaned containers from images that no longer exist
                    # This is safer as it only removes containers whose images are gone
                    docker ps -a --filter "status=exited" --format "{{.ID}} {{.Image}}" | while read id image; do
                        if ! docker image inspect "$image" >/dev/null 2>&1; then
                            docker rm -f "$id" 2>/dev/null || true
                        fi
                    done
                    success "Containers removed! Image preserved for quick recreation."
                    ;;
            esac
            echo
            docker system df
            exit 0
            ;;

        help|--help|-h)
            if docker image inspect "$IMAGE_NAME" &>/dev/null; then
                docker run --rm \
                    -u "$DOCKER_USER" \
                    --entrypoint /home/$DOCKER_USER/claude-wrapper \
                    "$IMAGE_NAME" --help | sed '1s/claude/claudebox/g'
                echo
                cecho "Added Options:" "$WHITE"
                echo -e "${CYAN}  -v, --verbose                   ${WHITE}Show detailed output"
                echo -e "${CYAN}  --dangerously-enable-sudo       ${WHITE}Enable sudo without password"
                echo -e "${CYAN}  --dangerously-disable-firewall  ${WHITE}Disable network restrictions"
                echo
                cecho "Added Commands:" "$WHITE"
                echo -e "  profile [names...]              Install language profiles"
                echo -e "  install <packages>              Install apt packages"
                echo -e "  save                            (Deprecated) Container state is auto-tracked"
                echo -e "  shell                           Open bash shell in container"
                echo -e "  info                            Show ClaudeBox container status"
                echo -e "  clean                           Clean up ClaudeBox resources (use 'clean --help' for options)"
                echo -e "  rebuild                         Rebuild the Docker image from scratch${NC}"
            else
                cecho "ClaudeBox - Claude Code Docker Environment" "$CYAN"
                echo
                warn "First run setup required!"
                echo "Run script without arguments first to build the Docker image."
            fi
            exit 0
            ;;

        info)
            shift
            cecho "ClaudeBox Profile Status" "$CYAN"
            echo

            local profile_dir="$HOME/.claudebox/profiles"
            if [[ ! -d "$profile_dir" ]] || [[ -z "$(ls -A "$profile_dir" 2>/dev/null)" ]]; then
                warn "No profiles configured yet."
                exit 0
            fi

            # Show all profiles
            local count
            count=$(ls -1 "$profile_dir"/*.ini 2>/dev/null | wc -l)
            info "Tracking $count project profile(s)"
            echo

            # Show each project's profiles
            for pfile in "$profile_dir"/*.ini; do
                [[ -f "$pfile" ]] || continue
                local proj_path
                proj_path=$(basename "$pfile" .ini | sed 's|-|/|g')
                cecho "/$proj_path:" "$YELLOW"

                # Show profiles
                local profiles=()
                readarray -t profiles < <(read_profile_section "$pfile" "profiles")
                if [[ ${#profiles[@]} -gt 0 ]]; then
                    echo "  Profiles: ${profiles[*]}"
                fi

                # Show packages
                local packages=()
                readarray -t packages < <(read_profile_section "$pfile" "packages")
                if [[ ${#packages[@]} -gt 0 ]]; then
                    echo "  Packages: ${packages[*]}"
                fi
                echo
            done

            # Show current project's profiles
            local current_profile_file
            current_profile_file=$(get_profile_file_path)
            if [[ -f "$current_profile_file" ]]; then
                cecho "Current project ($PROJECT_DIR):" "$GREEN"
                local current_profiles=()
                readarray -t current_profiles < <(read_profile_section "$current_profile_file" "profiles")
                if [[ ${#current_profiles[@]} -gt 0 ]]; then
                    echo "  Profiles: ${current_profiles[*]}"
                fi

                local current_packages=()
                readarray -t current_packages < <(read_profile_section "$current_profile_file" "packages")
                if [[ ${#current_packages[@]} -gt 0 ]]; then
                    echo "  Packages: ${current_packages[*]}"
                fi
            else
                info "Current project has no profiles configured."
            fi

            # Show running containers
            echo
            local running_containers
            running_containers=$(docker ps --filter "ancestor=$IMAGE_NAME" --format "table {{.ID}}\t{{.Status}}\t{{.Command}}" | tail -n +2)
            if [[ -n "$running_containers" ]]; then
                cecho "Running ClaudeBox containers:" "$YELLOW"
                echo "$running_containers"
            else
                info "No ClaudeBox containers currently running."
            fi

            exit 0
            ;;

        rebuild)
            shift  # Remove 'rebuild' from arguments
            warn "Rebuilding ClaudeBox Docker image..."

            # Check if image exists and remove it
            if docker image inspect "$IMAGE_NAME" &>/dev/null; then
                # Stop any running containers from this image first
                docker ps -a --filter "ancestor=$IMAGE_NAME" -q | xargs -r docker rm -f 2>/dev/null || true
                docker rmi -f "$IMAGE_NAME" 2>/dev/null || true
            fi

            # Continue to main execution which will detect missing image and rebuild
            ;;
    esac

    # Check for required environment variables
    if [[ -z "${ANTHROPIC_API_KEY:-}" ]]; then
        warn "ANTHROPIC_API_KEY not set. Claude features will be limited."
        warn "Set it with: export ANTHROPIC_API_KEY=your-key-here"
        echo
    fi
    
    # Setup workspace and global config
    setup_project_folder
    setup_global_mcp_config
    setup_claude_agent_command
    setup_apm_commands
    setup_apm_docs
    setup_agent_framework
    
    # First run help message
    local first_run_flag="$HOME/.claudebox/.not-first-run"
    if [[ ! -f "$first_run_flag" ]]; then
        echo
        cecho "🎉 Welcome to ClaudeBox!" "$CYAN"
        echo
        echo "Quick tips:"
        echo "  • Use 'claudebox profile' to see available development profiles"
        echo "  • Try '/apm-manager' in Claude to start structured project management"
        echo "  • Run 'claudebox help' for more commands"
        echo
        echo "Learn more at: https://github.com/samjtro/claudebox"
        echo
        touch "$first_run_flag"
    fi

    # Profile tracking
    mkdir -p "$HOME/.claudebox/profiles"

    # Check if we need to rebuild based on profiles
    local need_rebuild=false
    local current_profiles=()
    local profile_hash=""

    # Collect all profiles from all projects
    if [[ -d "$HOME/.claudebox/profiles" ]]; then
        for profile_file in "$HOME/.claudebox/profiles"/*.ini; do
            [[ -f "$profile_file" ]] || continue
            local profiles_from_file=()
            readarray -t profiles_from_file < <(read_profile_section "$profile_file" "profiles")
            for profile in "${profiles_from_file[@]}"; do
                profile=$(echo "$profile" | tr -d '[:space:]')
                [[ -z "$profile" ]] && continue
                # Add to array if not already present
                local found=false
                for p in "${current_profiles[@]}"; do
                    [[ "$p" == "$profile" ]] && found=true && break
                done
                [[ "$found" == "false" ]] && current_profiles+=("$profile")
            done
        done

        # Create a hash of current profiles
        if [[ ${#current_profiles[@]} -gt 0 ]]; then
            profile_hash=$(printf '%s\n' "${current_profiles[@]}" | sort | sha256sum | cut -d' ' -f1)
        fi
    fi

    # Check if image exists and if profiles match
    if docker image inspect "$IMAGE_NAME" >/dev/null 2>&1; then
        # Get the profile hash from the image labels
        local image_profile_hash
        image_profile_hash=$(docker inspect "$IMAGE_NAME" --format '{{index .Config.Labels "claudebox.profiles"}}' 2>/dev/null || echo "")

        if [[ "$profile_hash" != "$image_profile_hash" ]]; then
            warn "Profiles have changed. Rebuilding image..."
            warn "Current profiles: ${current_profiles[*]}"
            docker rmi -f "$IMAGE_NAME" 2>/dev/null || true
            need_rebuild=true
        fi
    else
        need_rebuild=true
    fi

    # Build image if needed
    if [[ "$need_rebuild" == "true" ]] || ! docker image inspect "$IMAGE_NAME" >/dev/null 2>&1; then
        logo
        local dockerfile
        # Use current directory instead of /tmp to avoid macOS xattr issues
        local build_timestamp=$(date +%s)
        dockerfile="$(pwd)/.claudebox-dockerfile.$build_timestamp.tmp"
        local firewall_script="$(pwd)/.claudebox-firewall.$build_timestamp.tmp"
        local entrypoint_script="$(pwd)/.claudebox-entrypoint.$build_timestamp.tmp"
        trap "rm -f '$dockerfile' '$firewall_script' '$entrypoint_script'" EXIT

        # Create firewall script
        cat > "$firewall_script" <<'FIREWALL_SCRIPT'
#!/bin/bash
set -euo pipefail
if [ "${DISABLE_FIREWALL:-false}" = "true" ]; then
    echo "Firewall disabled, skipping setup"
    rm -f "$0"
    exit 0
fi
iptables -F OUTPUT 2>/dev/null || true
iptables -F INPUT 2>/dev/null || true
iptables -A OUTPUT -p udp --dport 53 -j ACCEPT
iptables -A OUTPUT -p tcp --dport 53 -j ACCEPT
iptables -A INPUT -p udp --sport 53 -j ACCEPT
iptables -A OUTPUT -o lo -j ACCEPT
iptables -A INPUT -i lo -j ACCEPT
iptables -A OUTPUT -s 127.0.0.0/8 -d 127.0.0.0/8 -j ACCEPT
iptables -A INPUT -s 127.0.0.0/8 -d 127.0.0.0/8 -j ACCEPT
iptables -A OUTPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
# Default allowed domains
DEFAULT_DOMAINS="api.anthropic.com console.anthropic.com statsig.anthropic.com sentry.io"

# Read additional domains from allowlist file if it exists
ALLOWED_DOMAINS="$DEFAULT_DOMAINS"
# Try project-specific allowlist first, then fall back to global
if [ -f /home/USERNAME/.claudebox-project/firewall/allowlist ]; then
    echo "Loading project-specific allowlist"
    while IFS= read -r domain; do
        # Skip comments and empty lines
        [[ "$domain" =~ ^#.* ]] && continue
        [[ -z "$domain" ]] && continue
        # Remove wildcards for now (*.example.com becomes example.com)
        domain="${domain#\*.}"
        ALLOWED_DOMAINS="$ALLOWED_DOMAINS $domain"
    done < /home/USERNAME/.claudebox-project/firewall/allowlist
elif [ -f /home/USERNAME/.claudebox/allowlist ]; then
    echo "Loading global allowlist from .claudebox/allowlist"
    while IFS= read -r domain; do
        # Skip comments and empty lines
        [[ "$domain" =~ ^#.* ]] && continue
        [[ -z "$domain" ]] && continue
        # Remove wildcards for now (*.example.com becomes example.com)
        domain="${domain#\*.}"
        ALLOWED_DOMAINS="$ALLOWED_DOMAINS $domain"
    done < /home/USERNAME/.claudebox/allowlist
fi

if command -v ipset >/dev/null 2>&1; then
    ipset destroy allowed-domains 2>/dev/null || true
    ipset create allowed-domains hash:net
    ipset destroy allowed-ips 2>/dev/null || true
    ipset create allowed-ips hash:net

    for domain in $ALLOWED_DOMAINS; do
        # Check if it's an IP range
        if [[ "$domain" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+/[0-9]+$ ]]; then
            ipset add allowed-ips $domain 2>/dev/null || true
        else
            # It's a domain, resolve it
            ips=$(getent hosts $domain 2>/dev/null | awk '{print $1}')
            for ip in $ips; do
                ipset add allowed-domains $ip 2>/dev/null || true
            done
        fi
    done
    iptables -A OUTPUT -m set --match-set allowed-domains dst -j ACCEPT
    iptables -A OUTPUT -m set --match-set allowed-ips dst -j ACCEPT
else
    # Fallback without ipset
    for domain in $ALLOWED_DOMAINS; do
        if [[ "$domain" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+/[0-9]+$ ]]; then
            iptables -A OUTPUT -d $domain -j ACCEPT
        else
            # Resolve domain to IPs
            ips=$(getent hosts $domain 2>/dev/null | awk '{print $1}')
            for ip in $ips; do
                iptables -A OUTPUT -d $ip -j ACCEPT
            done
        fi
    done
fi
iptables -P OUTPUT DROP
iptables -P INPUT DROP
echo "Firewall initialized with Anthropic-only access"
rm -f "$0"
FIREWALL_SCRIPT
        sed -i.bak "s/USERNAME/$DOCKER_USER/g" "$firewall_script" && rm -f "$firewall_script.bak"

        # Create entrypoint script
        cat > "$entrypoint_script" <<'ENTRYPOINT_SCRIPT'
#!/bin/bash
ENABLE_SUDO=false
DISABLE_FIREWALL=false
while [[ $# -gt 0 ]]; do
    case "$1" in
        --dangerously-enable-sudo) ENABLE_SUDO=true; shift ;;
        --dangerously-disable-firewall) DISABLE_FIREWALL=true; shift ;;
        *) break ;;
    esac
done
if [ "$ENABLE_SUDO" = "true" ]; then
    usermod -aG sudo USERNAME
    echo 'USERNAME ALL=(ALL) NOPASSWD:ALL' > /etc/sudoers.d/dockeruser
    chmod 0440 /etc/sudoers.d/dockeruser
fi
export DISABLE_FIREWALL
if [ ! -f /home/USERNAME/.firewall-initialized ]; then
    if [ -f /home/USERNAME/init-firewall.sh ]; then
        su USERNAME -c "bash /home/USERNAME/init-firewall.sh"
        touch /home/USERNAME/.firewall-initialized
    fi
fi
# Auto install packages from install.list
install_list="/home/USERNAME/.claudebox-project/install.list"
installed_file="/home/USERNAME/.claudebox-project/.installed"
if [ -f "$install_list" ]; then
    if [ ! -f "$installed_file" ] || [ "$install_list" -nt "$installed_file" ]; then
        DEBIAN_FRONTEND=noninteractive apt-get update
        cat "$install_list" | grep -v '^#' | grep -v '^$' | xargs -r apt-get install -y
        apt-get clean && rm -rf /var/lib/apt/lists/*
        cp "$install_list" "$installed_file"
    fi
fi
# For Python profile, install UV and setup venv
if [ -f /home/USERNAME/.claudebox-project/profiles/.has_python ]; then
    if [ ! -d /home/USERNAME/.claudebox-project/.venv ]; then
        if ! command -v uv >/dev/null 2>&1; then
            curl -LsSf https://astral.sh/uv/install.sh | sh
            export PATH="/root/.cargo/bin:$PATH"
        fi
        if command -v uv >/dev/null 2>&1; then
            su - USERNAME -c "uv venv /home/USERNAME/.claudebox-project/.venv"
            if [ -f /workspace/pyproject.toml ]; then
                su - USERNAME -c "cd /workspace && uv sync"
            else
                su - USERNAME -c "uv pip install --python /home/USERNAME/.claudebox-project/.venv/bin/python ipython black pylint mypy flake8 pytest ruff"
            fi
        fi

        for shell_rc in /home/USERNAME/.zshrc /home/USERNAME/.bashrc; do
            if ! grep -q "source /home/USERNAME/.claudebox-project/.venv/bin/activate" "$shell_rc"; then
                echo 'if [ -f /home/USERNAME/.claudebox-project/.venv/bin/activate ]; then source /home/USERNAME/.claudebox-project/.venv/bin/activate; fi' >> "$shell_rc"
            fi
        done
    fi
fi

cd /home/USERNAME
# Check if we're in shell mode
if [[ "$1" == "--shell-mode" ]]; then
    shift  # Remove --shell-mode flag
    exec su USERNAME -c "source /home/USERNAME/.nvm/nvm.sh && cd /workspace && exec /bin/zsh"
else
    # Build command with properly quoted arguments
    cmd="cd /workspace && /home/USERNAME/claude-wrapper"
    for arg in "$@"; do
        # Escape single quotes in the argument
        escaped_arg=$(echo "$arg" | sed "s/'/'\\\\''/g")
        cmd="$cmd '$escaped_arg'"
    done
    exec su USERNAME -c "$cmd"
fi
ENTRYPOINT_SCRIPT
        sed -i.bak "s/USERNAME/$DOCKER_USER/g" "$entrypoint_script" && rm -f "$entrypoint_script.bak"

        cat > "$dockerfile" <<'DOCKERFILE'
FROM debian:bookworm
ARG USER_ID GROUP_ID USERNAME NODE_VERSION

RUN echo '#!/bin/sh\nexit 101' > /usr/sbin/policy-rc.d && chmod +x /usr/sbin/policy-rc.d

# Install locales first to fix locale warnings
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq locales && \
    echo "en_US.UTF-8 UTF-8" > /etc/locale.gen && \
    locale-gen en_US.UTF-8 && \
    rm -rf /var/lib/apt/lists/*

# Set locale environment variables
ENV LANG=en_US.UTF-8 \
    LANGUAGE=en_US:en \
    LC_ALL=en_US.UTF-8

RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq curl gnupg ca-certificates sudo git iptables ipset && \
    apt-get clean

RUN groupadd -g $GROUP_ID $USERNAME || true && \
    useradd -m -u $USER_ID -g $GROUP_ID -s /bin/bash $USERNAME

# Persist bash history
RUN SNIPPET="export PROMPT_COMMAND='history -a' && export HISTFILE=/commandhistory/.bash_history" \
    && mkdir /commandhistory \
    && touch /commandhistory/.bash_history \
    && chown -R $USERNAME /commandhistory

RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq \
    build-essential git wget curl unzip file vim nano \
    jq make less rsync openssh-client \
    procps sudo fzf zsh man-db gnupg2 \
    iptables ipset iproute2 dnsutils aggregate && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

RUN curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg && \
    chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg && \
    echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | tee /etc/apt/sources.list.d/github-cli.list > /dev/null && \
    apt update -qq && \
    apt install gh -y -qq && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

RUN DELTA_VERSION=$(curl -s https://api.github.com/repos/dandavison/delta/releases/latest | grep -Po '"tag_name": "\K[^"]*') && \
    ARCH=$(dpkg --print-architecture) && \
    wget -q https://github.com/dandavison/delta/releases/download/${DELTA_VERSION}/git-delta_${DELTA_VERSION}_${ARCH}.deb && \
    dpkg -i git-delta_${DELTA_VERSION}_${ARCH}.deb && \
    rm git-delta_${DELTA_VERSION}_${ARCH}.deb

USER $USERNAME
WORKDIR /home/$USERNAME

RUN sh -c "$(wget -O- https://github.com/deluan/zsh-in-docker/releases/download/v1.2.0/zsh-in-docker.sh)" -- \
    -p git \
    -p fzf \
    -a "source /usr/share/doc/fzf/examples/key-bindings.zsh" \
    -a "source /usr/share/doc/fzf/examples/completion.zsh" \
    -a "export PROMPT_COMMAND='history -a' && export HISTFILE=/commandhistory/.bash_history" \
    -a 'export HISTFILE=$HOME/.history/.zsh_history' \
    -a 'export HISTSIZE=10000' \
    -a 'export SAVEHIST=10000' \
    -a 'setopt HIST_IGNORE_DUPS' \
    -a 'setopt SHARE_HISTORY' \
    -a 'mkdir -p $HOME/.history' \
    -a 'export NVM_DIR="$HOME/.nvm"' \
    -a '[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"' \
    -a '[ -s "$NVM_DIR/bash_completion" ] && \. "$NVM_DIR/bash_completion"' \
    -x

RUN curl -LsSf https://astral.sh/uv/install.sh | sh
RUN echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc && \
    echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.zshrc

RUN git config --global core.pager delta && \
    git config --global interactive.diffFilter "delta --color-only" && \
    git config --global delta.navigate true && \
    git config --global delta.light false && \
    git config --global delta.side-by-side true

# Set DEVCONTAINER environment variable to help with orientation
ENV DEVCONTAINER=true

# Set the default shell to zsh rather than sh
ENV SHELL=/bin/zsh

ENV NVM_DIR="/home/$USERNAME/.nvm"
RUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash

RUN bash -c "source $NVM_DIR/nvm.sh && \
    if [[ \"$NODE_VERSION\" == '--lts' ]]; then \
        nvm install --lts && \
        nvm alias default 'lts/*'; \
    else \
        nvm install $NODE_VERSION && \
        nvm alias default $NODE_VERSION; \
    fi && \
    nvm use default"

RUN echo 'export NVM_DIR="$HOME/.nvm"' >> ~/.bashrc && \
    echo '[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"' >> ~/.bashrc && \
    echo '[ -s "$NVM_DIR/bash_completion" ] && \. "$NVM_DIR/bash_completion"' >> ~/.bashrc

RUN bash -c "source $NVM_DIR/nvm.sh && \
    nvm use default && \
    npm install -g @anthropic-ai/claude-code"

# Copy firewall script
COPY .claudebox-firewall.*.tmp /home/$USERNAME/init-firewall.sh

RUN chmod +x ~/init-firewall.sh

# Install profile packages as separate layers for better caching
USER root
DOCKERFILE

        # Add profile installations based on current profiles
        if [[ ${#current_profiles[@]} -gt 0 ]]; then
            info "Building with profiles: ${current_profiles[*]}"
            for profile in "${current_profiles[@]}"; do
                case "$profile" in
                    c)
                        cat >> "$dockerfile" <<'DOCKERFILE'
# C/C++ Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq build-essential gcc g++ gdb valgrind cmake ninja-build clang clang-format clang-tidy cppcheck doxygen libboost-all-dev autoconf automake libtool pkg-config libcmocka-dev libcmocka0 lcov libncurses5-dev libncursesw5-dev && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

DOCKERFILE
                        ;;
                    python)
    cat >> "$dockerfile" <<'DOCKERFILE'
# Python Development Profile (uv-managed Python only)
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq build-essential libffi-dev libssl-dev && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

DOCKERFILE
                        ;;
                    rust)
                        cat >> "$dockerfile" <<'DOCKERFILE'
# Rust Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq curl build-essential pkg-config libssl-dev && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

USER $USERNAME
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y && \
    echo 'source $HOME/.cargo/env' >> ~/.bashrc && \
    echo 'source $HOME/.cargo/env' >> ~/.zshrc
USER root

DOCKERFILE
                        ;;
                    go)
                        cat >> "$dockerfile" <<'DOCKERFILE'
# Go Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq wget git build-essential && \
    apt-get clean && rm -rf /var/lib/apt/lists/* && \
    GO_VERSION="1.21.5" && \
    wget -q "https://go.dev/dl/go${GO_VERSION}.linux-amd64.tar.gz" && \
    tar -C /usr/local -xzf "go${GO_VERSION}.linux-amd64.tar.gz" && \
    rm "go${GO_VERSION}.linux-amd64.tar.gz" && \
    echo 'export PATH=$PATH:/usr/local/go/bin' >> /etc/profile.d/go.sh

DOCKERFILE
                        ;;
                    javascript)
                        cat >> "$dockerfile" <<'DOCKERFILE'
# JavaScript Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq build-essential python3 && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

DOCKERFILE
                        ;;
                    java)
                        cat >> "$dockerfile" <<'DOCKERFILE'
# Java Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq openjdk-17-jdk maven gradle ant && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

DOCKERFILE
                        ;;
                    openwrt)
                        cat >> "$dockerfile" <<'DOCKERFILE'
# OpenWRT Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq build-essential gcc g++ make git wget unzip sudo file python3 python3-distutils rsync libncurses5-dev zlib1g-dev gawk gettext libssl-dev xsltproc libelf-dev libtool automake autoconf ccache subversion swig time qemu-system-arm qemu-system-aarch64 qemu-system-mips qemu-system-x86 qemu-utils && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

DOCKERFILE
                        ;;
                    ruby|php|database|devops|web|embedded|datascience|security|ml)
                        warn "Profile $profile not yet implemented in cached build"
                        ;;
                    *)
                        warn "Unknown profile: $profile"
                        ;;
                esac
            done
        fi

        # Add label with profile hash
        echo "# Label the image with the profile hash for change detection" >> "$dockerfile"
        echo "LABEL claudebox.profiles=\"$profile_hash\"" >> "$dockerfile"
        echo "" >> "$dockerfile"
        cat >> "$dockerfile" <<'DOCKERFILE'

USER $USERNAME

RUN bash -c "source $NVM_DIR/nvm.sh && claude --version"

RUN echo '#!/bin/bash' > ~/claude-wrapper && \
   echo 'export NVM_DIR="$HOME/.nvm"' >> ~/claude-wrapper && \
   echo '[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"' >> ~/claude-wrapper && \
   echo 'nvm use default >/dev/null 2>&1' >> ~/claude-wrapper && \
   echo '# Use MCP config if it exists' >> ~/claude-wrapper && \
   echo 'if [ -f "$HOME/.claudebox/.mcp.json" ]; then' >> ~/claude-wrapper && \
   echo '    exec claude --mcp-config "$HOME/.claudebox/.mcp.json" "$@"' >> ~/claude-wrapper && \
   echo 'else' >> ~/claude-wrapper && \
   echo '    exec claude "$@"' >> ~/claude-wrapper && \
   echo 'fi' >> ~/claude-wrapper && \
   chmod +x ~/claude-wrapper

WORKDIR /workspace

USER root
# Copy entrypoint script
COPY .claudebox-entrypoint.*.tmp /usr/local/bin/docker-entrypoint

RUN chmod +x /usr/local/bin/docker-entrypoint

ENTRYPOINT ["/usr/local/bin/docker-entrypoint"]
DOCKERFILE

        run_docker_build "$dockerfile"

        echo -e "\n${GREEN}Complete!${NC}\n"
        success "Docker image '$IMAGE_NAME' built!"

        echo
        cecho "ClaudeBox Setup Complete!" "$CYAN"
        echo
        cecho "Quick Start:" "$GREEN"
        echo -e "  ${YELLOW}claudebox [options]${NC}        # Launch Claude CLI"
        echo
        cecho "Power Features:" "$GREEN"
        echo -e "  ${YELLOW}claudebox profile${NC}                # See all language profiles"
        echo -e "  ${YELLOW}claudebox profile c openwrt${NC}      # Install C + OpenWRT tools"
        echo -e "  ${YELLOW}claudebox profile python ml${NC}      # Install Python + ML stack"
        echo -e "  ${YELLOW}claudebox install <packages>${NC}     # Install additional apt packages"
        echo -e "  ${YELLOW}claudebox shell${NC}                  # Open bash shell in container"
        echo
        cecho "Security:" "$GREEN"
        echo -e "  Network firewall: ON by default (Anthropic recommended)"
        echo -e "  Sudo access: OFF by default"
        echo
        cecho "Maintenance:" "$GREEN"
        echo -e "  ${YELLOW}claudebox clean --help${NC}            # See all cleanup options"
        echo
        cecho "Just install the profile you need and start coding!" "$PURPLE"
       exit 0
   fi

   # Run container
   local extra_mounts=()

   # Ensure .claudebox exists with proper permissions
   if [[ ! -d "$HOME/.claudebox" ]]; then
       mkdir -p "$HOME/.claudebox"
   fi

   # Fix permissions if needed
   if [[ ! -w "$HOME/.claudebox" ]]; then
       warn "Fixing .claudebox permissions..."
       sudo chown -R "$USER:$USER" "$HOME/.claudebox" || true
   fi

   # Create project-specific allowlist if it doesn't exist
   local project_folder_name
   project_folder_name=$(get_project_folder_name "$PROJECT_DIR")
   local allowlist_file="$HOME/.claudebox/$project_folder_name/firewall/allowlist"

   if [[ ! -f "$allowlist_file" ]]; then
       mkdir -p "$(dirname "$allowlist_file")"
       info "Creating default firewall allowlist for project..."
       cat > "$allowlist_file" <<'EOF'
# ClaudeBox Firewall Allowlist
# Lines starting with # are comments
# Add one domain or IP range per line
#
# Default domains (always allowed):
# - api.anthropic.com
# - console.anthropic.com
# - statsig.anthropic.com
# - sentry.io

# ====================
# GitHub.com
# ====================
github.com
api.github.com
raw.githubusercontent.com
ssh.github.com
avatars.githubusercontent.com
codeload.github.com
objects.githubusercontent.com
pipelines.actions.githubusercontent.com
ghcr.io
pkg-containers.githubusercontent.com

# ====================
# GitLab.com
# ====================
gitlab.com
api.gitlab.com
registry.gitlab.com
uploads.gitlab.com
gitlab.io
*.gitlab.io
*.s3.amazonaws.com
*.amazonaws.com

# ====================
# Bitbucket.org
# ====================
bitbucket.org
api.bitbucket.org
altssh.bitbucket.org
bbuseruploads.s3.amazonaws.com
bitbucket-pipelines-prod-us-west-2.s3.amazonaws.com
bitbucket-pipelines-prod-us-east-1.s3.amazonaws.com
bitbucket-pipelines-prod-eu-west-1.s3.amazonaws.com

# ====================
# Atlassian IP Ranges (Bitbucket Cloud)
# ====================
104.192.136.0/21
185.166.140.0/22
13.200.41.128/25
18.246.31.128/25

# ====================
# Optional (Git LFS, Assets)
# ====================
github-cloud.s3.amazonaws.com
github-releases.githubusercontent.com
github-production-release-asset-2e65be.s3.amazonaws.com
EOF
       success "Created default allowlist"
   fi

   # Create new container with --rm
   info "Starting claudebox for $PROJECT_DIR..."

   # Check if we have a TTY
   local tty_flag=""
   if [ -t 0 ] && [ -t 1 ]; then
       tty_flag="-it"
   else
       tty_flag="-i"
   fi

   # Get project-specific folder
   local project_folder_name
   project_folder_name=$(get_project_folder_name "$PROJECT_DIR")

   docker run $tty_flag --rm \
       -w /workspace \
       -v "$PROJECT_DIR":/workspace \
       -v "$HOME/.claudebox/$project_folder_name":/home/$DOCKER_USER/.claudebox-project \
       -v "$HOME/.claudebox":/home/$DOCKER_USER/.claudebox \
       -v "$HOME/.claude.json":/home/$DOCKER_USER/.claude.json \
       -v "$HOME/.claude":/home/$DOCKER_USER/.claude \
       -v "$HOME/.npmrc":/home/$DOCKER_USER/.npmrc:ro \
       -v "$HOME/.ssh":/home/$DOCKER_USER/.ssh:ro \
       -e "NODE_ENV=${NODE_ENV:-production}" \
       -e "ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}" \
       -e "OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}" \
       -e "OPENROUTER_DEFAULT_MODEL=${OPENROUTER_DEFAULT_MODEL:-}" \
       -e "NODE_OPTIONS=--max-old-space-size=4096" \
       -e "POWERLEVEL9K_DISABLE_GITSTATUS=true" \
       -e "CLAUDEBOX_PROJECT_DIR=$PROJECT_DIR" \
       "${extra_mounts[@]}" \
       --cap-add NET_ADMIN \
       --cap-add NET_RAW \
       "$IMAGE_NAME" "${DEFAULT_FLAGS[@]}" "${RUN_ARGS[@]:-$@}"
}

main "$@"
